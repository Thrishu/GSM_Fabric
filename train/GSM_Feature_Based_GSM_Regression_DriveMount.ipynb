{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef6ee93",
   "metadata": {},
   "source": [
    "# Fabric GSM Prediction ‚Äî Feature-Based Regression (CatBoost)\n",
    "\n",
    "This notebook trains a regression model to predict fabric GSM using engineered features extracted from images. Images are used only for visualization/sanity checks; the model input is purely tabular features. The design targets an MAE within ¬±5‚Äì10 GSM if features are sufficiently informative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed37d3",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Imports & Configuration\n",
    "- Structured, publication-ready code with seeded randomness\n",
    "- Uses CatBoost with optional GPU (devices='0'), MAE loss\n",
    "- Paths configurable for local or Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment & Base Path: local vs Colab (Google Drive)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/fabric_gsm_pipeline')\n",
    "    print('Running in Colab; mounted Google Drive.')\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "    # Use workspace root (two levels up from this notebook's train folder)\n",
    "    BASE_PATH = Path.cwd()  # Adjust if you want a specific folder\n",
    "    print('Running locally; using workspace path:', BASE_PATH)\n",
    "\n",
    "# Optional: override via env var GSM_BASE_PATH\n",
    "env_base = os.environ.get('GSM_BASE_PATH')\n",
    "if env_base:\n",
    "    BASE_PATH = Path(env_base)\n",
    "    print('BASE_PATH overridden by GSM_BASE_PATH:', BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151857a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Core Configuration\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "    print('CatBoost import failed; please install catboost.')\n",
    "\n",
    "# Plotting style\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Prefer datasets that include a 'split' column (train/val/test).\n",
    "def bp(*parts):\n",
    "    return BASE_PATH.joinpath(*parts)\n",
    "\n",
    "DATASET_CANDIDATES = [\n",
    "    bp('split_feature_dataset', 'dataset_all.csv'),\n",
    "    bp('feature_extracted_dataset', 'dataset_with_features.csv'),\n",
    "    bp('data', 'augmented_features_dataset', 'dataset_with_features.csv'),\n",
    "    bp('Dataset', 'dataset.csv'),\n",
    "    bp('data', 'combined_dataset', 'dataset.csv')\n",
    "]\n",
    "\n",
    "def pick_dataset_path(candidates):\n",
    "    for p in candidates:\n",
    "        if Path(p).exists():\n",
    "            return Path(p)\n",
    "    return Path(candidates[0])  # default fallback\n",
    "\n",
    "DATASET_PATH = pick_dataset_path(DATASET_CANDIDATES)\n",
    "print(f'Using dataset: {DATASET_PATH}')\n",
    "\n",
    "# Image search roots for sanity checks\n",
    "IMAGE_DIR_CANDIDATES = [\n",
    "    bp('split_feature_dataset', 'train', 'images'),\n",
    "    bp('split_feature_dataset', 'val', 'images'),\n",
    "    bp('split_feature_dataset', 'test', 'images'),\n",
    "    bp('feature_extracted_dataset', 'images'),\n",
    "    bp('preprocessed_dataset', 'images'),\n",
    "    bp('Dataset', 'images'),\n",
    "    bp('data', 'augmented_dataset', 'images'),\n",
    "    bp('data', 'augmented_features_dataset', 'images'),\n",
    "    bp('data', 'feature_extracted_dataset', 'images'),\n",
    "    bp('data', 'preprocessed_dataset', 'images')\n",
    "]\n",
    "\n",
    "# Columns to explicitly exclude from features\n",
    "EXCLUDE_COLS = set([\n",
    "    'image_name', 'source', 'augmentation', 'original_image', 'split',\n",
    "    'gsm'  # target kept separately\n",
    "])\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = bp('train')\n",
    "VIS_DIR = bp('feature_extracted_dataset', 'visualizations') if bp('feature_extracted_dataset').exists() else bp('train', 'visualizations')\n",
    "MODEL_DIR = bp('Model')\n",
    "PREDICTIONS_PATH = bp('train', 'predictions_gsm_feature_catboost.csv')\n",
    "MODEL_PATH = MODEL_DIR / 'gsm_feature_catboost.cbm'\n",
    "Path(VIS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def has_gpu_for_catboost() -> bool:\n",
    "    if not CATBOOST_AVAILABLE:\n",
    "        return False\n",
    "    try:\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', '') not in ['', '-1']:\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import subprocess\n",
    "        res = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=3, shell=False)\n",
    "        if res.returncode == 0 and len(res.stdout.decode().strip()) > 0:\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "GPU_AVAILABLE = has_gpu_for_catboost()\n",
    "print(f'GPU available for CatBoost: {GPU_AVAILABLE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45a401",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Loading & Inspection\n",
    "- Load CSV and inspect shape/dtypes\n",
    "- Identify numeric vs non-numeric columns\n",
    "- Explicitly exclude meta columns from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1457bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "assert Path(DATASET_PATH).exists(), f'Dataset not found: {DATASET_PATH}'\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "print('Shape:', df.shape)\n",
    "print('Columns:', list(df.columns))\n",
    "\n",
    "# Basic checks\n",
    "assert 'gsm' in df.columns, 'Target column gsm is missing.'\n",
    "\n",
    "# Dtypes summary\n",
    "print('\n",
    "Dtypes:')\n",
    "print(df.dtypes)\n",
    "\n",
    "# Identify numeric vs non-numeric columns\n",
    "numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "non_numeric_cols = [c for c in df.columns if c not in numeric_cols]\n",
    "print('\n",
    "Numeric columns (count={}):'.format(len(numeric_cols)))\n",
    "print(numeric_cols[:20], '...')\n",
    "print('\n",
    "Non-numeric columns (count={}):'.format(len(non_numeric_cols)))\n",
    "print(non_numeric_cols)\n",
    "\n",
    "# Exclusions\n",
    "print('\n",
    "Explicitly excluding columns from features:', sorted(EXCLUDE_COLS))\n",
    "\n",
    "# Preview\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1e0ca",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Train / Validation Split (CRITICAL)\n",
    "- Use the existing `split` column\n",
    "- Ensure augmented samples remain with their original image's split\n",
    "- Prevent leakage between train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d058516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'split' column exists\n",
    "assert 'split' in df.columns, 'The dataset must contain a split column.'\n",
    "\n",
    "# Group by original identifier to enforce consistent split. Prefer 'original_image' then fallback to 'image_name'.\n",
    "group_key = 'original_image' if 'original_image' in df.columns else ('image_name' if 'image_name' in df.columns else None)\n",
    "assert group_key is not None, 'Dataset should have either original_image or image_name for grouping.'\n",
    "\n",
    "# Detect inconsistent splits within a group and fix by majority vote\n",
    "inconsistent_groups = []\n",
    "def majority_split(s):\n",
    "    return s.value_counts().idxmax()\n",
    "\n",
    "df['split_fixed'] = df['split']\n",
    "for gid, g in df.groupby(group_key):\n",
    "    if g['split'].nunique() > 1:\n",
    "        inconsistent_groups.append(gid)\n",
    "        maj = majority_split(g['split'])\n",
    "        df.loc[g.index, 'split_fixed'] = maj\n",
    "\n",
    "if inconsistent_groups:\n",
    "    print(f'Found {len(inconsistent_groups)} groups with inconsistent splits; fixed via majority assignment.')\n",
    "else:\n",
    "    print('No inconsistent group splits detected.')\n",
    "\n",
    "# Define train/val based on fixed splits; test is unused for training\n",
    "train_df = df[df['split_fixed'].str.lower() == 'train'].copy()\n",
    "val_df = df[df['split_fixed'].str.lower() == 'val'].copy()\n",
    "print(f'Train: {train_df.shape}, Val: {val_df.shape}')\n",
    "\n",
    "# Sanity: ensure groups do not cross splits\n",
    "train_groups = set(train_df[group_key].unique())\n",
    "val_groups = set(val_df[group_key].unique())\n",
    "intersection = train_groups.intersection(val_groups)\n",
    "assert len(intersection) == 0, f'Leakage detected! {len(intersection)} groups appear in both train and val.'\n",
    "print('No group leakage between train and val.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701e41d",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Feature Cleaning\n",
    "- Keep only numeric features\n",
    "- Drop near-constant features\n",
    "- Handle missing values safely\n",
    "- Optional: prune highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature set: numeric columns excluding meta/target\n",
    "feature_cols = [c for c in numeric_cols if c not in EXCLUDE_COLS and c != 'gsm']\n",
    "print(f'Initial numeric feature count: {len(feature_cols)}')\n",
    "\n",
    "# Separate X and y\n",
    "X_train = train_df[feature_cols].copy()\n",
    "X_val = val_df[feature_cols].copy()\n",
    "y_train_raw = train_df['gsm'].astype(float).copy()\n",
    "y_val_raw = val_df['gsm'].astype(float).copy()\n",
    "\n",
    "# Handle NaNs: CatBoost can handle NaNs, but imputing reduces surprises in plots/stats\n",
    "median_vals = X_train.median()\n",
    "X_train = X_train.fillna(median_vals)\n",
    "X_val = X_val.fillna(median_vals)  # use train medians for val\n",
    "\n",
    "# Drop near-constant features (variance threshold)\n",
    "vt = VarianceThreshold(threshold=1e-8)\n",
    "_ = vt.fit(X_train)\n",
    "mask = vt.get_support()\n",
    "feature_cols_vt = [col for col, keep in zip(feature_cols, mask) if keep]\n",
    "print(f'After variance filter: {len(feature_cols_vt)} features (dropped {len(feature_cols) - len(feature_cols_vt)})')\n",
    "\n",
    "X_train = X_train[feature_cols_vt]\n",
    "X_val = X_val[feature_cols_vt]\n",
    "\n",
    "# Optional: correlation-based pruning to reduce multicollinearity\n",
    "corr_threshold = 0.98\n",
    "corr = X_train.corr(numeric_only=True).abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n",
    "feature_cols_pruned = [c for c in feature_cols_vt if c not in to_drop]\n",
    "print(f'Correlation pruning: dropped {len(to_drop)} features above {corr_threshold}')\n",
    "\n",
    "X_train = X_train[feature_cols_pruned]\n",
    "X_val = X_val[feature_cols_pruned]\n",
    "print(f'Final feature count: {len(feature_cols_pruned)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152f64c",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Target Engineering (IMPORTANT)\n",
    "We transform the target using $y=\\log(1+\\text{GSM})$ to reduce skew and stabilize the training dynamics. Predictions are inverse-transformed via $\\exp(y)-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log1p transformation to target\n",
    "y_train = np.log1p(y_train_raw.values)\n",
    "y_val = np.log1p(y_val_raw.values)\n",
    "\n",
    "print('Target stats (raw GSM):')\n",
    "print(pd.Series(y_val_raw).describe())\n",
    "print('\n",
    "Target stats (log1p GSM):')\n",
    "print(pd.Series(y_val).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98e56e",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Model Selection\n",
    "We use `CatBoostRegressor` because it performs strongly on structured/tabular data, models feature interactions well, and is stable on smaller datasets.\n",
    "- Loss: MAE (aligned with GSM error target)\n",
    "- GPU support: `task_type='GPU'`, `devices='0'` when available\n",
    "- CPU fallback when GPU is unavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5cf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare CatBoost Pools\n",
    "feature_names = list(X_train.columns)\n",
    "train_pool = Pool(X_train, label=y_train, feature_names=feature_names)\n",
    "val_pool = Pool(X_val, label=y_val, feature_names=feature_names)\n",
    "\n",
    "# Model parameters\n",
    "cat_params = {\n",
    "    'loss_function': 'MAE',\n",
    "    'eval_metric': 'MAE',\n",
    "    'random_seed': SEED,\n",
    "    'depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'iterations': 2000,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'use_best_model': True,\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    cat_params.update({'task_type': 'GPU', 'devices': '0'})\n",
    "else:\n",
    "    cat_params.update({'task_type': 'CPU'})\n",
    "\n",
    "print('CatBoost params:', cat_params)\n",
    "\n",
    "# Initialize model\n",
    "assert CATBOOST_AVAILABLE, 'CatBoost is not installed; pip install catboost'\n",
    "model = CatBoostRegressor(**cat_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8d831",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Training Strategy\n",
    "- Train on train split and validate on val split\n",
    "- Use early stopping and best-iteration selection\n",
    "- No cross-validation abuse to avoid leakage from augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with early stopping\n",
    "start_time = time.time()\n",
    "model.fit(train_pool, eval_set=val_pool)\n",
    "train_time = time.time() - start_time\n",
    "print(f'Training completed in {train_time:.2f} seconds')\n",
    "print('Best iteration:', model.get_best_iteration())\n",
    "print('Best score (val MAE):', model.get_best_score().get('validation', {}).get('MAE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bbdce",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Evaluation\n",
    "- Inverse-transform predictions back to GSM\n",
    "- Report MAE, RMSE, and mean GSM\n",
    "- Plot Actual vs Predicted and error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afa886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and invert transformation\n",
    "val_pred_log = model.predict(val_pool)\n",
    "val_pred = np.expm1(val_pred_log)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_val_raw, val_pred)\n",
    "rmse = math.sqrt(mean_squared_error(y_val_raw, val_pred))\n",
    "mean_gsm = float(np.mean(y_val_raw))\n",
    "print(f'Validation MAE: {mae:.3f} GSM')\n",
    "print(f'Validation RMSE: {rmse:.3f} GSM')\n",
    "print(f'Mean GSM (val): {mean_gsm:.3f}')\n",
    "\n",
    "# Scatter: Actual vs Predicted\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(y_val_raw, val_pred, s=18, alpha=0.7, edgecolor='none')\n",
    "min_v = float(min(y_val_raw.min(), val_pred.min()))\n",
    "max_v = float(max(y_val_raw.max(), val_pred.max()))\n",
    "plt.plot([min_v, max_v], [min_v, max_v], 'r--', lw=1)\n",
    "plt.title('Actual vs Predicted GSM (Validation)')\n",
    "plt.xlabel('Actual GSM')\n",
    "plt.ylabel('Predicted GSM')\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIS_DIR / 'actual_vs_pred_val.png', dpi=120)\n",
    "plt.show()\n",
    "\n",
    "# Error distribution\n",
    "errors = np.abs(y_val_raw.values - val_pred)\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.histplot(errors, bins=30, kde=True, color='steelblue')\n",
    "plt.title('Absolute Error Distribution (Validation)')\n",
    "plt.xlabel('Absolute Error (GSM)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIS_DIR / 'error_distribution_val.png', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28b712",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Feature Importance\n",
    "We inspect CatBoost's feature importances to understand which engineered features drive GSM predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5fc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = model.get_feature_importance(train_pool)\n",
    "imp_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "imp_df = imp_df.sort_values('importance', ascending=False)\n",
    "display(imp_df.head(20))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "topn = 20\n",
    "sns.barplot(y=imp_df['feature'].head(topn), x=imp_df['importance'].head(topn), color='teal')\n",
    "plt.title(f'Top {topn} Feature Importances (CatBoost)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIS_DIR / 'feature_importance_top20.png', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1611a6",
   "metadata": {},
   "source": [
    "Interpretation: Higher importances indicate stronger relationships with GSM. Expect structural features (e.g., weft/warp counts, spacing stats), yarn uniformity, and frequency/texture descriptors to rank highly. If color features dominate, revisit lighting normalization or feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2ce7b",
   "metadata": {},
   "source": [
    "## üîü Visual Sanity Check\n",
    "Randomly sample 10 validation images; display image with actual and predicted GSM and absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f96360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to find an image by name across known directories\n",
    "def find_image_path(image_name: str, split_hint: str = None):\n",
    "    candidates = []\n",
    "    if split_hint:\n",
    "        candidates.append(bp('split_feature_dataset', split_hint, 'images') / image_name)\n",
    "    for root in IMAGE_DIR_CANDIDATES:\n",
    "        candidates.append(root / image_name)\n",
    "    for p in candidates:\n",
    "        if Path(p).exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# Prepare display data\n",
    "val_display = val_df.copy()\n",
    "val_display['pred_gsm'] = val_pred\n",
    "val_display['abs_error'] = np.abs(val_display['gsm'] - val_display['pred_gsm'])\n",
    "sample_n = min(10, len(val_display))\n",
    "sample = val_display.sample(sample_n, random_state=SEED)\n",
    "\n",
    "# Plot grid\n",
    "ncols = 5\n",
    "nrows = math.ceil(sample_n / ncols)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*3.5, nrows*3.5))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "\n",
    "for ax in axes[sample_n:]:\n",
    "    ax.axis('off')\n",
    "\n",
    "for i, (_, row) in enumerate(sample.iterrows()):\n",
    "    ax = axes[i]\n",
    "    img_name = row['image_name'] if 'image_name' in row else None\n",
    "    img_path = find_image_path(str(img_name) if img_name is not None else '', split_hint='val') if img_name else None\n",
    "    if img_path and Path(img_path).exists():\n",
    "        try:\n",
    "            img = plt.imread(str(img_path))\n",
    "            ax.imshow(img)\n",
    "        except Exception:\n",
    "            ax.text(0.5, 0.5, 'Image load failed', ha='center', va='center')\n",
    "            ax.set_facecolor('#f7f7f7')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
    "        ax.set_facecolor('#f7f7f7')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    title = f\"Actual: {row['gsm']:.1f} | Pred: {row['pred_gsm']:.1f} | |Err|: {row['abs_error']:.1f}\"\n",
    "    ax.set_title(title, fontsize=9)\n",
    "\n",
    "plt.suptitle('Validation Samples: Visual Sanity Check', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIS_DIR / 'val_visual_sanity_check.png', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b396ec",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Error Analysis\n",
    "Identify the worst 5 predictions and discuss likely causes: structural ambiguity (mixed weave patterns), yarn overlaps, motion blur, illumination variances, or features missing key discriminants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst 5 predictions\n",
    "worst = val_display.sort_values('abs_error', ascending=False).head(5)[['image_name', 'gsm', 'pred_gsm', 'abs_error']]\n",
    "print('Worst 5 (by absolute error):')\n",
    "display(worst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55c3b9",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Model Saving\n",
    "Save the trained CatBoost model (.cbm) and export validation predictions to CSV for further analysis/plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and predictions\n",
    "Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "model.save_model(str(MODEL_PATH))\n",
    "print(f'Saved CatBoost model to: {MODEL_PATH}')\n",
    "\n",
    "pred_out_cols = [group_key]\n",
    "if 'image_name' in val_df.columns:\n",
    "    pred_out_cols.append('image_name')\n",
    "pred_out_cols += ['gsm', 'pred_gsm', 'abs_error']\n",
    "pred_out = val_display[pred_out_cols].copy()\n",
    "pred_out.to_csv(PREDICTIONS_PATH, index=False)\n",
    "print(f'Saved validation predictions to: {PREDICTIONS_PATH}')\n",
    "\n",
    "# Save feature list and config for reproducibility\n",
    "meta = {\n",
    "    'dataset_path': str(DATASET_PATH),\n",
    "    'features_used': feature_names,\n",
    "    'final_features': list(X_train.columns),\n",
    "    'seed': SEED,\n",
    "    'gpu_used': GPU_AVAILABLE,\n",
    "    'catboost_params': cat_params,\n",
    "    'val_metrics': {'MAE': float(mae), 'RMSE': float(rmse), 'Mean_GSM_val': float(mean_gsm)}\n",
    "}\n",
    "with open(bp('train', 'training_metadata.json'), 'w') as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print('Saved training metadata to train/training_metadata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85521dd3",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- This pipeline respects pre-defined splits to avoid leakage from augmentations.\n",
    "- Target log1p helps stabilize training; always invert before reporting.\n",
    "- If MAE is above 10 GSM, investigate feature extraction quality, illumination normalization, and split integrity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
