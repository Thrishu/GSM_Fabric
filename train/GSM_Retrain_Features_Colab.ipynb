{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e611ac9",
   "metadata": {},
   "source": [
    "## GSM Fine-Tuning with Augmented Features Dataset\n",
    "\n",
    "This notebook fine-tunes a pre-trained GSM regressor using the augmented features dataset with:\n",
    "- **Small learning rate** to preserve previously learned features\n",
    "- **All extracted features** (weft/warp counts, texture, color, etc.)\n",
    "- **Error tolerance of ¬±5 GSM** based on weave pattern closeness\n",
    "- **Transfer learning** from pre-trained model mounted in Google Drive\n",
    "\n",
    "### Quick Start:\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4 preferred)\n",
    "2. Section 1: Mount Google Drive and configure paths\n",
    "3. Section 2: Load pre-trained model\n",
    "4. Section 3: Prepare augmented features dataset\n",
    "5. Section 4-6: Fine-tune model with small learning rate\n",
    "6. Section 7: Evaluate and save results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8499a",
   "metadata": {},
   "source": [
    "# Section 1: Setup and Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and Set Reproducibility\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"‚úÖ CUDA available: {gpu_name}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è  CUDA not available. Running on CPU.\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Google Drive mounted\")\n",
    "except Exception as e:\n",
    "    IN_COLAB = False\n",
    "    print(f\"‚ö†Ô∏è  Not in Colab environment: {e}\")\n",
    "    print(\"Using local paths instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Configuration and Paths\n",
    "# ============================================\n",
    "\n",
    "# FOR COLAB: Set these paths to your Google Drive locations\n",
    "if IN_COLAB:\n",
    "    DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
    "    PRETRAINED_MODEL_PATH = os.path.join(DRIVE_ROOT, \"path_to_pretrained_model.pt\")  # TODO: update this\n",
    "    OUTPUT_DIR = os.path.join(DRIVE_ROOT, \"GSM_Finetuned_Model\")\n",
    "else:\n",
    "    # LOCAL: Update these to your local paths\n",
    "    LOCAL_ROOT = r\"c:\\Users\\I769816\\Desktop\\GSM_fabric\\fabric_gsm_pipeline\"\n",
    "    PRETRAINED_MODEL_PATH = os.path.join(LOCAL_ROOT, \"Model\", \"best_model (1).pt\")\n",
    "    OUTPUT_DIR = os.path.join(LOCAL_ROOT, \"train\", \"finetuned_model\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset paths\n",
    "if IN_COLAB:\n",
    "    FEATURES_DATASET_PATH = os.path.join(DRIVE_ROOT, \"augmented_features_dataset\")\n",
    "else:\n",
    "    FEATURES_DATASET_PATH = os.path.join(LOCAL_ROOT, \"augmented_features_dataset\")\n",
    "\n",
    "# CSV files\n",
    "TRAIN_CSV = os.path.join(FEATURES_DATASET_PATH, \"dataset_train.csv\")\n",
    "VAL_CSV = os.path.join(FEATURES_DATASET_PATH, \"dataset_val.csv\")\n",
    "TEST_CSV = os.path.join(FEATURES_DATASET_PATH, \"dataset_test.csv\")\n",
    "IMAGES_DIR = os.path.join(FEATURES_DATASET_PATH, \"images\")\n",
    "\n",
    "print(\"üìÅ Configuration:\")\n",
    "print(f\"  Pretrained Model: {PRETRAINED_MODEL_PATH}\")\n",
    "print(f\"  Train CSV: {TRAIN_CSV}\")\n",
    "print(f\"  Output Dir: {OUTPUT_DIR}\")\n",
    "print(f\"  Images Dir: {IMAGES_DIR}\")\n",
    "\n",
    "# Verify paths exist\n",
    "assert os.path.exists(PRETRAINED_MODEL_PATH), f\"Model not found: {PRETRAINED_MODEL_PATH}\"\n",
    "assert os.path.exists(TRAIN_CSV), f\"Train CSV not found: {TRAIN_CSV}\"\n",
    "assert os.path.exists(VAL_CSV), f\"Val CSV not found: {VAL_CSV}\"\n",
    "assert os.path.exists(TEST_CSV), f\"Test CSV not found: {TEST_CSV}\"\n",
    "print(\"‚úÖ All paths verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888ff13",
   "metadata": {},
   "source": [
    "# Section 2: Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN Regressor Architecture (same as original)\n",
    "import timm\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, backbone_name='efficientnet_b3', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,\n",
    "            global_pool='avg'\n",
    "        )\n",
    "        in_feats = self.backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_feats, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        out = self.head(f).squeeze(1)\n",
    "        return out\n",
    "\n",
    "# Load the pre-trained model\n",
    "print(\"Loading pre-trained model...\")\n",
    "model = Regressor(backbone_name='efficientnet_b3', pretrained=False).to(device)\n",
    "\n",
    "checkpoint = torch.load(PRETRAINED_MODEL_PATH, map_location=device)\n",
    "\n",
    "# Handle different checkpoint formats\n",
    "if isinstance(checkpoint, dict) and 'model_state' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    print(f\"‚úÖ Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "    print(f\"   Previous Val MAE: {checkpoint.get('val_mae', 'N/A')}\")\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"‚úÖ Loaded model weights\")\n",
    "\n",
    "print(f\"üìä Model: {model.__class__.__name__}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6412b",
   "metadata": {},
   "source": [
    "# Section 3: Prepare Augmented Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd84e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files\n",
    "print(\"Loading dataset CSVs...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df = pd.read_csv(VAL_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_df)} samples\")\n",
    "print(f\"‚úÖ Val:   {len(val_df)} samples\")\n",
    "print(f\"‚úÖ Test:  {len(test_df)} samples\")\n",
    "\n",
    "# Display columns\n",
    "print(f\"\\nüìä Dataset Columns ({len(train_df.columns)}):\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "# Display basic stats\n",
    "print(f\"\\nüìà GSM Statistics:\")\n",
    "print(train_df['gsm'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature columns (exclude image_name, gsm, source, augmentation, original_image, split)\n",
    "exclude_cols = {'image_name', 'gsm', 'source', 'augmentation', 'original_image', 'split'}\n",
    "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "print(f\"üìä Feature Columns ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\n‚ö†Ô∏è  Missing values in train:\")\n",
    "missing = train_df[feature_cols].isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"  None!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f795e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Normalizing features...\")\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_df[feature_cols])\n",
    "val_features = scaler.transform(val_df[feature_cols])\n",
    "test_features = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "# Save scaler for later use\n",
    "import pickle\n",
    "scaler_path = os.path.join(OUTPUT_DIR, 'feature_scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"‚úÖ Scaler saved to {scaler_path}\")\n",
    "\n",
    "print(f\"\\nüìä Normalized feature shapes:\")\n",
    "print(f\"  Train: {train_features.shape}\")\n",
    "print(f\"  Val:   {val_features.shape}\")\n",
    "print(f\"  Test:  {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526bc3fc",
   "metadata": {},
   "source": [
    "# Section 4: Create Feature Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f81fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-based Dataset (no images, pure features)\n",
    "class FeaturesDataset(Dataset):\n",
    "    def __init__(self, features, targets, names=None):\n",
    "        \"\"\"\n",
    "        features: numpy array of shape (N, num_features)\n",
    "        targets: numpy array of shape (N,) with GSM values\n",
    "        names: optional list of image names\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.names = names if names is not None else ['unknown'] * len(features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx], self.names[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FeaturesDataset(\n",
    "    train_features,\n",
    "    train_df['gsm'].values,\n",
    "    train_df['image_name'].tolist()\n",
    ")\n",
    "\n",
    "val_dataset = FeaturesDataset(\n",
    "    val_features,\n",
    "    val_df['gsm'].values,\n",
    "    val_df['image_name'].tolist()\n",
    ")\n",
    "\n",
    "test_dataset = FeaturesDataset(\n",
    "    test_features,\n",
    "    test_df['gsm'].values,\n",
    "    test_df['image_name'].tolist()\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Datasets created\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val:   {len(val_dataset)} samples\")\n",
    "print(f\"  Test:  {len(test_dataset)} samples\")\n",
    "\n",
    "# Test a sample\n",
    "feat_sample, gsm_sample, name_sample = train_dataset[0]\n",
    "print(f\"\\nüìä Sample feature shape: {feat_sample.shape}, GSM: {gsm_sample.item():.2f}, Name: {name_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32  # Can be larger with features-only\n",
    "NUM_WORKERS = 0  # Features don't need multiprocessing\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders created\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753a8b0",
   "metadata": {},
   "source": [
    "# Section 5: Fine-tuning Configuration (Small Learning Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1659e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è  Fine-tuning Configuration:\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Early Stopping: DISABLED (will train full {EPOCHS} epochs)\")\n",
    "print(f\"  Best Model: Will be saved automatically\")\n",
    "print(f\"  Error Tolerance: ¬±{ERROR_TOLERANCE} GSM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128351e",
   "metadata": {},
   "source": [
    "# Section 6: Fine-tuning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab896cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Training Loop\n",
    "# ============================================\n",
    "\n",
    "best_val_mae = float('inf')\n",
    "history = defaultdict(list)\n",
    "\n",
    "best_ckpt_path = os.path.join(OUTPUT_DIR, 'best_finetuned_model.pt')\n",
    "\n",
    "print(\"üöÄ Starting fine-tuning (200 epochs, no early stopping)...\\\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # -------- Training --------\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for features, targets, _ in tqdm(train_loader, desc=f\"Epoch {epoch:3d}/{EPOCHS} [Train]\", leave=False):\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(features)\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # -------- Validation --------\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, targets, _ in val_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(features)\n",
    "            val_preds.append(preds.cpu())\n",
    "            val_trues.append(targets.cpu())\n",
    "\n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_trues = torch.cat(val_trues)\n",
    "\n",
    "    # Metrics\n",
    "    train_loss = float(np.mean(train_losses))\n",
    "    val_mae = float(torch.mean(torch.abs(val_preds - val_trues)))\n",
    "    val_rmse = float(torch.sqrt(torch.mean((val_preds - val_trues) ** 2)))\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "\n",
    "    # -------- Learning Rate Warmup --------\n",
    "    if epoch <= WARMUP_EPOCHS:\n",
    "        lr = LEARNING_RATE * epoch / max(1, WARMUP_EPOCHS)\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # -------- Logging --------\n",
    "    print(\n",
    "        f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.6f} | \"\n",
    "        f\"Val MAE: {val_mae:.4f} | \"\n",
    "        f\"Val RMSE: {val_rmse:.4f} | \"\n",
    "        f\"LR: {lr:.2e}\"\n",
    "    )\n",
    "\n",
    "    # -------- Save best checkpoint (No early stopping) --------\n",
    "    if val_mae < best_val_mae - 1e-4:\n",
    "        best_val_mae = val_mae\n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state': model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_mae': best_val_mae,\n",
    "                'val_rmse': val_rmse,\n",
    "                'train_loss': train_loss\n",
    "            },\n",
    "            best_ckpt_path\n",
    "        )\n",
    "        print(f\"  ‚úÖ Saved best model (Val MAE = {best_val_mae:.4f})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Best Val MAE achieved: {best_val_mae:.4f}\")\n",
    "print(f\"   Total epochs trained: {EPOCHS}\")\n",
    "print(f\"   Model saved to: {best_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['val_mae'], label='Val MAE', linewidth=2, color='orange')\n",
    "axes[1].plot(history['val_rmse'], label='Val RMSE', linewidth=2, color='red')\n",
    "axes[1].axhline(y=ERROR_TOLERANCE, color='green', linestyle='--', label=f'Error Tolerance (¬±{ERROR_TOLERANCE})')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Error')\n",
    "axes[1].set_title('Validation Metrics')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c6ab2",
   "metadata": {},
   "source": [
    "# Section 7: Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best fine-tuned model...\")\n",
    "checkpoint = torch.load(best_ckpt_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"   Val MAE: {checkpoint['val_mae']:.4f}\")\n",
    "print(f\"   Val RMSE: {checkpoint['val_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462551a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "model.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_trues = []\n",
    "test_names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, targets, names in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        preds = model(features)\n",
    "        test_preds.append(preds.cpu())\n",
    "        test_trues.append(targets.cpu())\n",
    "        test_names.extend(names)\n",
    "\n",
    "test_preds = torch.cat(test_preds).numpy()\n",
    "test_trues = torch.cat(test_trues).numpy()\n",
    "\n",
    "# Metrics\n",
    "test_mae = mean_absolute_error(test_trues, test_preds)\n",
    "test_rmse = np.sqrt(mean_squared_error(test_trues, test_preds))\n",
    "test_r2 = r2_score(test_trues, test_preds)\n",
    "\n",
    "# Error tolerance analysis\n",
    "test_errors = np.abs(test_preds - test_trues)\n",
    "within_tolerance = (test_errors <= ERROR_TOLERANCE).mean() * 100\n",
    "\n",
    "print(\"\\nüìä Test Set Metrics:\")\n",
    "print(f\"  MAE:  {test_mae:.4f} (Mean Absolute Error)\")\n",
    "print(f\"  RMSE: {test_rmse:.4f} (Root Mean Squared Error)\")\n",
    "print(f\"  R¬≤:   {test_r2:.4f}\")\n",
    "print(f\"\\n‚úÖ Within ¬±{ERROR_TOLERANCE} tolerance: {within_tolerance:.1f}% ({int(within_tolerance/100 * len(test_trues))} / {len(test_trues)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ab578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution by weave pattern (if available in data)\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['pred'] = test_preds\n",
    "test_df_copy['error'] = test_errors\n",
    "test_df_copy['within_tolerance'] = (test_errors <= ERROR_TOLERANCE).astype(int)\n",
    "\n",
    "print(\"\\nüìà Error Statistics by Source (Weave Pattern):\")\n",
    "if 'source' in test_df_copy.columns:\n",
    "    for source in test_df_copy['source'].unique():\n",
    "        mask = test_df_copy['source'] == source\n",
    "        src_errors = test_df_copy[mask]['error'].values\n",
    "        src_tolerance = test_df_copy[mask]['within_tolerance'].mean() * 100\n",
    "        print(f\"  {source:15s}: MAE={src_errors.mean():.4f}, Within tolerance={src_tolerance:.1f}%\")\n",
    "\n",
    "# Save detailed predictions\n",
    "output_csv = os.path.join(OUTPUT_DIR, 'test_predictions.csv')\n",
    "test_df_copy.to_csv(output_csv, index=False)\n",
    "print(f\"\\n‚úÖ Predictions saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Predictions vs Ground Truth\n",
    "axes[0, 0].scatter(test_trues, test_preds, alpha=0.6, s=50)\n",
    "min_val = min(test_trues.min(), test_preds.min())\n",
    "max_val = max(test_trues.max(), test_preds.max())\n",
    "axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].fill_between(\n",
    "    [min_val, max_val],\n",
    "    [min_val - ERROR_TOLERANCE, max_val - ERROR_TOLERANCE],\n",
    "    [min_val + ERROR_TOLERANCE, max_val + ERROR_TOLERANCE],\n",
    "    alpha=0.2, color='green', label=f'¬±{ERROR_TOLERANCE} Tolerance'\n",
    ")\n",
    "axes[0, 0].set_xlabel('Ground Truth GSM')\n",
    "axes[0, 0].set_ylabel('Predicted GSM')\n",
    "axes[0, 0].set_title(f'Predictions vs Ground Truth (R¬≤={test_r2:.4f})')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Error distribution\n",
    "axes[0, 1].hist(test_errors, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 1].axvline(ERROR_TOLERANCE, color='green', linestyle='--', linewidth=2, label=f'Tolerance={ERROR_TOLERANCE}')\n",
    "axes[0, 1].axvline(test_mae, color='red', linestyle='--', linewidth=2, label=f'MAE={test_mae:.4f}')\n",
    "axes[0, 1].set_xlabel('Absolute Error (GSM)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Error Distribution')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals\n",
    "residuals = test_trues - test_preds\n",
    "axes[1, 0].scatter(test_preds, residuals, alpha=0.6, s=50)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].fill_between([test_preds.min(), test_preds.max()], -ERROR_TOLERANCE, ERROR_TOLERANCE, alpha=0.2, color='green')\n",
    "axes[1, 0].set_xlabel('Predicted GSM')\n",
    "axes[1, 0].set_ylabel('Residuals (True - Pred)')\n",
    "axes[1, 0].set_title('Residual Plot')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Cumulative tolerance\n",
    "sorted_errors = np.sort(test_errors)\n",
    "cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100\n",
    "axes[1, 1].plot(sorted_errors, cumulative, linewidth=2)\n",
    "axes[1, 1].axvline(ERROR_TOLERANCE, color='green', linestyle='--', linewidth=2, label=f'¬±{ERROR_TOLERANCE} GSM')\n",
    "axes[1, 1].set_xlabel('Absolute Error (GSM)')\n",
    "axes[1, 1].set_ylabel('Cumulative %')\n",
    "axes[1, 1].set_title('Cumulative Error Distribution')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'evaluation_results.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Evaluation plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c65d40",
   "metadata": {},
   "source": [
    "# Section 8: Export and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514bbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final model and metrics\n",
    "final_model_path = os.path.join(OUTPUT_DIR, 'gsm_regressor_finetuned.pt')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"‚úÖ Model saved to {final_model_path}\")\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_summary = {\n",
    "    'train': {\n",
    "        'samples': len(train_df),\n",
    "        'final_loss': float(history['train_loss'][-1])\n",
    "    },\n",
    "    'val': {\n",
    "        'samples': len(val_df),\n",
    "        'mae': float(history['val_mae'][-1]),\n",
    "        'best_mae': best_val_mae\n",
    "    },\n",
    "    'test': {\n",
    "        'samples': len(test_df),\n",
    "        'mae': float(test_mae),\n",
    "        'rmse': float(test_rmse),\n",
    "        'r2': float(test_r2),\n",
    "        'within_tolerance': float(within_tolerance)\n",
    "    },\n",
    "    'config': {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'epochs': len(history['train_loss']),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'error_tolerance': ERROR_TOLERANCE,\n",
    "        'num_features': len(feature_cols)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "summary_path = os.path.join(OUTPUT_DIR, 'metrics_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "print(f\"‚úÖ Metrics saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34134371",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"üéØ FINE-TUNING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\\\nüìä Dataset:\")\n",
    "print(f\"  Train samples:  {len(train_df)} (with {len(train_df.columns)-6} extracted features)\")\n",
    "print(f\"  Val samples:    {len(val_df)}\")\n",
    "print(f\"  Test samples:   {len(test_df)}\")\n",
    "\n",
    "print(f\"\\\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"  Learning Rate:  {LEARNING_RATE} (Small for transfer learning)\")\n",
    "print(f\"  Total Epochs:   {EPOCHS} (No early stopping)\")\n",
    "print(f\"  Batch Size:     {BATCH_SIZE}\")\n",
    "print(f\"  Error tolerance: ¬±{ERROR_TOLERANCE} GSM\")\n",
    "\n",
    "print(f\"\\\\nüìà Results:\")\n",
    "print(f\"  Best Val MAE:   {best_val_mae:.4f}\")\n",
    "print(f\"  Test MAE:       {test_mae:.4f}\")\n",
    "print(f\"  Test RMSE:      {test_rmse:.4f}\")\n",
    "print(f\"  Test R¬≤:        {test_r2:.4f}\")\n",
    "print(f\"  ‚úÖ Within tolerance: {within_tolerance:.1f}%\")\n",
    "\n",
    "print(f\"\\\\nüíæ Output files:\")\n",
    "print(f\"  Best model:     {best_ckpt_path}\")\n",
    "print(f\"  Final model:    {final_model_path}\")\n",
    "print(f\"  Predictions:    {output_csv}\")\n",
    "print(f\"  Metrics:        {summary_path}\")\n",
    "print(f\"  Scaler:         {scaler_path}\")\n",
    "print(f\"  Plots:          {os.path.join(OUTPUT_DIR, 'training_curves.png')}\")\n",
    "print(f\"                  {os.path.join(OUTPUT_DIR, 'evaluation_results.png')}\")\n",
    "print(f\"\\\\n‚úÖ Fine-tuning complete! (200 epochs trained, best model saved)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
