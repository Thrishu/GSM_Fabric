{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1115044b",
   "metadata": {},
   "source": [
    "# Fabric GSM: All-In-One Colab Notebook\n",
    "Train GSM regression (100 epochs) and visualize weave patterns, warp/weft densities, and texture features (GLCM, LBP, Gabor). GPU-accelerated feature extraction with MobileNetV2. Designed for Google Colab T4 GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4399790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Colab Environment Setup (Pip + GPU)\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip -q install tensorflow==2.13.0 keras==2.13.0 scikit-learn scikit-image opencv-python-headless pyyaml openpyxl pandas numpy matplotlib pillow\n",
    "    print('Installed dependencies for Colab.')\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print('GPUs available:', gpus)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b05140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Reproducibility and Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "import cv2, skimage, sklearn\n",
    "print({'tensorflow': tf.__version__, 'numpy': np.__version__, 'pandas': pd.__version__, 'opencv': cv2.__version__, 'skimage': skimage.__version__, 'sklearn': sklearn.__version__})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Colab Drive Mount + Paths\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    project_root = Path('/content/drive/MyDrive/GSM_fabric/fabric_gsm_pipeline')\n",
    "else:\n",
    "    project_root = Path('.')\n",
    "print('Project root:', project_root)\n",
    "dataset_root = project_root / 'data' / 'FabricNet'\n",
    "excel_path = dataset_root / 'FabricNet_parameters.xlsx'\n",
    "assert dataset_root.exists(), 'Dataset folder not found'\n",
    "assert excel_path.exists(), 'Excel file not found'\n",
    "import sys as _sys\n",
    "_sys.path.insert(0, str(project_root))\n",
    "from src.utils.config import load_config\n",
    "config = load_config(project_root / 'configs' / 'config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555dd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load Dataset via FabricNetDataset\n",
    "from src.data.fabricnet_loader import FabricNetDataset\n",
    "fabricnet = FabricNetDataset(dataset_root=dataset_root)\n",
    "samples = fabricnet.get_all_samples()\n",
    "df = pd.DataFrame([{\n",
    "    'image_path': str(p),\n",
    "    'specific_weight': gsm,\n",
    "    'image_id': meta['image_id'],\n",
    "    'warp': meta.get('warp'),\n",
    "    'weft': meta.get('weft'),\n",
    "    'texture': meta.get('texture'),\n",
    "} for p, gsm, meta in samples])\n",
    "print('Loaded samples:', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Visual Grid of 12 Samples\n",
    "import math\n",
    "n_show = 12\n",
    "sel = df.sample(n=n_show, random_state=SEED)\n",
    "cols = 4\n",
    "rows = math.ceil(n_show/cols)\n",
    "plt.figure(figsize=(12,9))\n",
    "for i, r in enumerate(sel.itertuples(), 1):\n",
    "    img = cv2.imread(r.image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.subplot(rows, cols, i)\n",
    "    plt.imshow(img); plt.axis('off')\n",
    "    plt.title(f\"ID {r.image_id} GSM {r.specific_weight:.1f}\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b57e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Stratified Train/Val/Test Split (qcut by GSM)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "n_bins = 5\n",
    "df['gsm_bin'] = pd.qcut(df['specific_weight'], q=n_bins, labels=False, duplicates='drop')\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=config.get('data', {}).get('test_ratio', 0.15), random_state=SEED)\n",
    "train_idx, test_idx = next(splitter.split(df, df['gsm_bin']))\n",
    "df_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "df_temp = df.iloc[test_idx].reset_index(drop=True)\n",
    "val_ratio = config.get('data', {}).get('val_ratio', 0.15)\n",
    "test_ratio = config.get('data', {}).get('test_ratio', 0.15)\n",
    "temp_ratio = val_ratio + test_ratio\n",
    "val_size = val_ratio / temp_ratio\n",
    "splitter2 = StratifiedShuffleSplit(n_splits=1, test_size=1 - val_size, random_state=SEED)\n",
    "val_idx, test_idx2 = next(splitter2.split(df_temp, df_temp['gsm_bin']))\n",
    "df_val = df_temp.iloc[val_idx].reset_index(drop=True)\n",
    "df_test = df_temp.iloc[test_idx2].reset_index(drop=True)\n",
    "print(f\"Split sizes -> train: {len(df_train)}, val: {len(df_val)}, test: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Preprocessing + Feature Extraction (Texture + Deep)\n",
    "from src.preprocessing.image_preprocessor import ImagePreprocessor\n",
    "from src.features.texture_features import TextureFeatureExtractor\n",
    "from src.features.deep_features import DeepFeatureExtractor\n",
    "\n",
    "pre_cfg = config.get('preprocessing', {})\n",
    "preprocessor = ImagePreprocessor(\n",
    "    target_size=tuple(config.get('data', {}).get('image_size', [224, 224])),\n",
    "    enable_clahe=pre_cfg.get('enable_clahe', True),\n",
    "    clahe_clip_limit=pre_cfg.get('clahe_clip_limit', 2.0),\n",
    "    clahe_tile_size=tuple(pre_cfg.get('clahe_tile_size', [8, 8])),\n",
    "    normalization_mode=pre_cfg.get('normalization', 'minmax'),\n",
    ")\n",
    "\n",
    "tex_cfg = config.get('texture_features', {})\n",
    "texture_extractor = TextureFeatureExtractor(\n",
    "    glcm_enabled=tex_cfg.get('glcm', {}).get('enable', True),\n",
    "    glcm_distances=tex_cfg.get('glcm', {}).get('distances', [1,3]),\n",
    "    glcm_angles=tex_cfg.get('glcm', {}).get('angles', [0,45,90,135]),\n",
    "    glcm_levels=tex_cfg.get('glcm', {}).get('levels', 256),\n",
    "    glcm_metrics=tex_cfg.get('glcm', {}).get('metrics', ['contrast','homogeneity','energy','correlation']),\n",
    "    lbp_enabled=tex_cfg.get('lbp', {}).get('enable', True),\n",
    "    lbp_radius=tex_cfg.get('lbp', {}).get('radius', 3),\n",
    "    lbp_n_points=tex_cfg.get('lbp', {}).get('n_points', 8),\n",
    "    lbp_method=tex_cfg.get('lbp', {}).get('method', 'uniform'),\n",
    "    lbp_n_bins=tex_cfg.get('lbp', {}).get('n_bins', 59),\n",
    ")\n",
    "\n",
    "deep_cfg = config.get('deep_features', {})\n",
    "deep_extractor = DeepFeatureExtractor(\n",
    "    model_type=deep_cfg.get('model_type', 'MobileNetV2'),\n",
    "    input_shape=(224,224,3),\n",
    "    weights=deep_cfg.get('weights', 'imagenet'),\n",
    "    pooling=deep_cfg.get('pooling', 'global_average'),\n",
    "    preprocessing_mode=deep_cfg.get('preprocessing_mode', 'tf'),\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "def compute_features(df_split):\n",
    "    tex_list, deep_list, y_list, id_list = [], [], [], []\n",
    "    for row in df_split.itertuples():\n",
    "        img_path = Path(row.image_path)\n",
    "        img = preprocessor.preprocess(img_path)\n",
    "        tex_feats = texture_extractor.extract_features(img)\n",
    "        deep_feats = deep_extractor.extract_features(img)\n",
    "        tex_list.append(tex_feats)\n",
    "        deep_list.append(deep_feats)\n",
    "        y_list.append(float(row.specific_weight))\n",
    "        id_list.append(int(row.image_id))\n",
    "    return (np.vstack(tex_list), np.vstack(deep_list), np.array(y_list, dtype=np.float32), np.array(id_list, dtype=np.int32))\n",
    "\n",
    "tex_train, deep_train, y_train, id_train = compute_features(df_train)\n",
    "tex_val, deep_val, y_val, id_val = compute_features(df_val)\n",
    "tex_test, deep_test, y_test, id_test = compute_features(df_test)\n",
    "print('Texture shapes:', tex_train.shape, tex_val.shape, tex_test.shape)\n",
    "print('Deep shapes:', deep_train.shape, deep_val.shape, deep_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed227c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Feature Fusion + Standardization\n",
    "from src.features.feature_fusion import FeatureFusion\n",
    "X_train_raw = np.concatenate([tex_train, deep_train], axis=1)\n",
    "X_val_raw = np.concatenate([tex_val, deep_val], axis=1)\n",
    "X_test_raw = np.concatenate([tex_test, deep_test], axis=1)\n",
    "feature_dim = X_train_raw.shape[1]\n",
    "fusion_cfg = config.get('feature_fusion', {})\n",
    "fusion = FeatureFusion(\n",
    "    scaler_type=fusion_cfg.get('scaler_type', 'standard'),\n",
    "    save_scaler=True,\n",
    "    scaler_path=project_root / fusion_cfg.get('scaler_path', 'models/feature_scaler.pkl'),\n",
    ")\n",
    "X_train = fusion.fit_transform(X_train_raw)\n",
    "X_val = fusion.transform(X_val_raw)\n",
    "X_test = fusion.transform(X_test_raw)\n",
    "print('Fused feature dim:', feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Keras MLP Regressor (100 Epochs)\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "mlp = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(feature_dim,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1, activation='linear'),\n",
    "])\n",
    "mlp.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = int(config.get('training', {}).get('batch_size', 32))\n",
    "history = mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Evaluation and Save Predictions/Model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "y_pred = mlp.predict(X_test).reshape(-1)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "r2 = float(r2_score(y_test, y_pred))\n",
    "print({'mae': float(mae), 'rmse': rmse, 'r2': r2})\n",
    "pred_path = project_root / config.get('evaluation', {}).get('predictions_path', 'results/predictions.csv')\n",
    "pred_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame({'image_id': id_test, 'true_gsm': y_test, 'pred_gsm': y_pred}).to_csv(pred_path, index=False)\n",
    "print('Saved predictions to', pred_path)\n",
    "model_path = project_root / config.get('paths', {}).get('model_save_path', 'models/fabric_gsm_regressor.h5')\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "mlp.save(model_path)\n",
    "print('Saved model to', model_path)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('True GSM'); plt.ylabel('Predicted GSM')\n",
    "plt.title(f'Test R2={r2:.3f}, MAE={mae:.2f}, RMSE={rmse:.2f}')\n",
    "lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
    "plt.plot(lims, lims, 'r--'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccfa2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Visual Feature Sections: Edges/Hough, Orientation, FFT, GLCM/LBP, Gabor, Structure Tensor\n",
    "import numpy as np\n",
    "def visualize_edges_hough(image_path):\n",
    "    img_bgr = cv2.imread(str(image_path))\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    gray_blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    edges = cv2.Canny(gray_blur, 50, 150)\n",
    "    lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi/180, threshold=80, minLineLength=30, maxLineGap=10)\n",
    "    overlay = img_rgb.copy()\n",
    "    if lines is not None:\n",
    "        for l in lines[:,0]:\n",
    "            x1,y1,x2,y2 = l\n",
    "            cv2.line(overlay, (x1,y1), (x2,y2), (255,0,0), 2)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1); plt.imshow(img_rgb); plt.title('Original'); plt.axis('off')\n",
    "    plt.subplot(1,3,2); plt.imshow(edges, cmap='gray'); plt.title('Canny Edges'); plt.axis('off')\n",
    "    plt.subplot(1,3,3); plt.imshow(overlay); plt.title('Hough Lines Overlay'); plt.axis('off')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def orientation_histogram(image_path, n_bins=36):\n",
    "    img_bgr = cv2.imread(str(image_path))\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (224,224), interpolation=cv2.INTER_CUBIC)\n",
    "    gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    magnitude = np.sqrt(gx*gx + gy*gy)\n",
    "    angle = (np.arctan2(gy, gx) * 180.0/np.pi) % 180.0\n",
    "    hist, bins = np.histogram(angle, bins=n_bins, range=(0,180), weights=magnitude)\n",
    "    centers = 0.5*(bins[:-1]+bins[1:])\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.bar(centers, hist, width=180/n_bins)\n",
    "    plt.title('Orientation Histogram'); plt.xlabel('Angle (deg)'); plt.ylabel('Weighted count'); plt.show()\n",
    "    peak_angles = centers[np.argsort(hist)[-3:][::-1]]\n",
    "    return {'hist': hist, 'centers': centers, 'peak_angles_deg': peak_angles}\n",
    "\n",
    "def dominant_frequency(signal):\n",
    "    sig = signal.astype(np.float32); sig = sig - sig.mean()\n",
    "    N = len(sig)\n",
    "    fft = np.fft.rfft(sig)\n",
    "    mag = np.abs(fft); mag[0] = 0\n",
    "    peak_idx = np.argmax(mag)\n",
    "    freq = peak_idx / N\n",
    "    period_px = (1.0 / freq) if freq > 0 else np.inf\n",
    "    return freq, period_px, mag\n",
    "\n",
    "def estimate_warp_weft(image_path):\n",
    "    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (224,224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = img.astype(np.float32)/255.0\n",
    "    row_mean = img.mean(axis=1)\n",
    "    col_mean = img.mean(axis=0)\n",
    "    warp_freq, warp_period, warp_mag = dominant_frequency(row_mean)\n",
    "    weft_freq, weft_period, weft_mag = dominant_frequency(col_mean)\n",
    "    fig, axs = plt.subplots(2,2, figsize=(10,6))\n",
    "    axs[0,0].plot(row_mean); axs[0,0].set_title(f'Row Mean (Warp) period~{warp_period:.1f}px')\n",
    "    axs[0,1].plot(warp_mag); axs[0,1].set_title(f'Warp FFT mag (peak {warp_freq:.4f})')\n",
    "    axs[1,0].plot(col_mean); axs[1,0].set_title(f'Col Mean (Weft) period~{weft_period:.1f}px')\n",
    "    axs[1,1].plot(weft_mag); axs[1,1].set_title(f'Weft FFT mag (peak {weft_freq:.4f})')\n",
    "    plt.tight_layout(); plt.show()\n",
    "    return {'warp_freq': warp_freq, 'warp_period_px': warp_period, 'weft_freq': weft_freq, 'weft_period_px': weft_period}\n",
    "\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "def show_glcm_lbp(image_path, distances=[1,3], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256, lbp_radius=3):\n",
    "    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (224,224), interpolation=cv2.INTER_CUBIC)\n",
    "    img_norm = (img.astype(np.float32)/255.0)\n",
    "    quantized = (img_norm*(levels-1)).astype(np.uint8)\n",
    "    glcm = graycomatrix(quantized, distances=distances, angles=angles, levels=levels, symmetric=True, normed=True)\n",
    "    metrics = {\n",
    "        'contrast': graycoprops(glcm, 'contrast'),\n",
    "        'homogeneity': graycoprops(glcm, 'homogeneity'),\n",
    "        'energy': graycoprops(glcm, 'energy'),\n",
    "        'correlation': graycoprops(glcm, 'correlation'),\n",
    "    }\n",
    "    lbp = local_binary_pattern(img_norm, P=lbp_radius*8, R=lbp_radius, method='uniform')\n",
    "    n_bins = 59\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins))\n",
    "    hist = hist.astype(np.float32); hist = hist / (hist.sum() + 1e-8)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1); plt.imshow(img, cmap='gray'); plt.title('Image'); plt.axis('off')\n",
    "    plt.subplot(1,3,2); plt.imshow(glcm[:,:,0,0], cmap='magma'); plt.title('GLCM (d=1, θ=0)'); plt.colorbar(); plt.axis('off')\n",
    "    plt.subplot(1,3,3); plt.bar(np.arange(n_bins), hist); plt.title('LBP Histogram')\n",
    "    plt.tight_layout(); plt.show()\n",
    "    return metrics, hist\n",
    "\n",
    "from skimage.filters import gabor\n",
    "def show_gabor_bank(image_path, frequencies=[0.1, 0.2, 0.3], thetas=[0, np.pi/4, np.pi/2, 3*np.pi/4]):\n",
    "    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (224,224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = img.astype(np.float32)/255.0\n",
    "    fig, axs = plt.subplots(len(frequencies), len(thetas), figsize=(12,8))\n",
    "    for i,f in enumerate(frequencies):\n",
    "        for j,t in enumerate(thetas):\n",
    "            real, imag = gabor(img, frequency=f, theta=t)\n",
    "            axs[i,j].imshow(real, cmap='gray')\n",
    "            axs[i,j].set_title(f'f={f}, θ={int(t*180/np.pi)}°')\n",
    "            axs[i,j].axis('off')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "from skimage.feature import structure_tensor, structure_tensor_eigvals\n",
    "def show_orientation_field(image_path):\n",
    "    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (224,224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = img.astype(np.float32)/255.0\n",
    "    Axx, Axy, Ayy = structure_tensor(img, sigma=1.0)\n",
    "    e1, e2 = structure_tensor_eigvals(Axx, Axy, Ayy)\n",
    "    orientation = 0.5*np.arctan2(2*Axy, Axx - Ayy)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(orientation, cmap='twilight'); plt.colorbar(); plt.title('Orientation Field'); plt.axis('off'); plt.show()\n",
    "\n",
    "# Demo on one sample\n",
    "sample_path = df_train.sample(1, random_state=SEED)['image_path'].iloc[0]\n",
    "visualize_edges_hough(sample_path)\n",
    "oh = orientation_histogram(sample_path)\n",
    "print('Top orientation peaks (deg):', oh['peak_angles_deg'])\n",
    "est = estimate_warp_weft(sample_path)\n",
    "print('Warp/Weft estimation:', est)\n",
    "_ = show_glcm_lbp(sample_path)\n",
    "show_gabor_bank(sample_path)\n",
    "show_orientation_field(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dce506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Dataset-wide Visual Feature Summary and Correlations\n",
    "def summarize_features(df_split, limit=None):\n",
    "    rows = []\n",
    "    count = 0\n",
    "    for r in df_split.itertuples():\n",
    "        if limit and count >= limit:\n",
    "            break\n",
    "        p = r.image_path\n",
    "        est = estimate_warp_weft(p)\n",
    "        oh = orientation_histogram(p)\n",
    "        metrics, hist = show_glcm_lbp(p)\n",
    "        contrast_mean = float(metrics['contrast'].mean())\n",
    "        homog_mean = float(metrics['homogeneity'].mean())\n",
    "        energy_mean = float(metrics['energy'].mean())\n",
    "        corr_mean = float(metrics['correlation'].mean())\n",
    "        lbp_entropy = float(-(hist*np.log(hist+1e-8)).sum())\n",
    "        rows.append({\n",
    "            'image_id': r.image_id,\n",
    "            'gsm': float(r.specific_weight),\n",
    "            'warp_freq': est['warp_freq'],\n",
    "            'weft_freq': est['weft_freq'],\n",
    "            'warp_period_px': est['warp_period_px'],\n",
    "            'weft_period_px': est['weft_period_px'],\n",
    "            'orientation_peak_deg_1': float(oh['peak_angles_deg'][0]),\n",
    "            'orientation_peak_deg_2': float(oh['peak_angles_deg'][1]),\n",
    "            'orientation_peak_deg_3': float(oh['peak_angles_deg'][2]),\n",
    "            'glcm_contrast_mean': contrast_mean,\n",
    "            'glcm_homogeneity_mean': homog_mean,\n",
    "            'glcm_energy_mean': energy_mean,\n",
    "            'glcm_correlation_mean': corr_mean,\n",
    "            'lbp_entropy': lbp_entropy,\n",
    "        })\n",
    "        count += 1\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "summary_df = summarize_features(df, limit=130)\n",
    "out_path = project_root / 'results' / 'features_visual_summary.csv'\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "summary_df.to_csv(out_path, index=False)\n",
    "print('Saved feature summary to', out_path)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.scatter(summary_df['warp_freq'], summary_df['gsm']); plt.xlabel('Warp freq'); plt.ylabel('GSM')\n",
    "plt.subplot(1,2,2); plt.scatter(summary_df['weft_freq'], summary_df['gsm']); plt.xlabel('Weft freq'); plt.ylabel('GSM')\n",
    "plt.tight_layout(); plt.show()\n",
    "print('Spearman correlations:')\n",
    "print(summary_df[['warp_freq','weft_freq','warp_period_px','weft_period_px','glcm_contrast_mean','glcm_homogeneity_mean','lbp_entropy','gsm']].corr(method='spearman'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
