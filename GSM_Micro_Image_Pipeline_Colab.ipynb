{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658f9710",
   "metadata": {},
   "source": [
    "## Quick Start (in Colab)\n",
    "\n",
    "1. Runtime → Change runtime type → GPU (prefer T4).\n",
    "2. Run Section 2 to install packages (first time only).\n",
    "3. In Section 3, upload kaggle.json and set `KAGGLE_DATASET`.\n",
    "4. In Section 4, set `CSV_PATH`, `IMAGE_DIR`, `IMAGE_COL`, and confirm `TARGET_COL`.\n",
    "5. Run through Sections 5–8 to split and (optionally) augment to ~1000 samples.\n",
    "6. Optionally annotate/correct labels in Section 9 and re-run splits if changed.\n",
    "7. Inspect features (Section 10) and then train the CNN (Sections 11–13).\n",
    "8. Review metrics and visuals (Section 14) and Grad-CAM (Section 15).\n",
    "9. Optionally enable K-Fold (Section 16) for robustness.\n",
    "10. Evaluate on test + export model and predictions (Sections 17–18)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c13374",
   "metadata": {},
   "source": [
    "# GSM Microscopy Pipeline (Colab-Ready)\n",
    "\n",
    "This notebook downloads a Kaggle microscopy dataset, scales a small set (~130) of images to ~1000 via augmentation, supports optional label annotation, extracts features, and trains a strong GSM regressor (and optional classifier) on T4 GPU. It includes rich visualizations for augmentations, embeddings, and model explanations (Grad-CAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df57c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Check GPU and Set Reproducibility\n",
    "import os, random, math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"CUDA available: {gpu_name}\")\n",
    "    # Prefer mixed precision on T4\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CUDA not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc95d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Install and Import Dependencies\n",
    "# If running in Colab, uncomment the next cell to install packages\n",
    "# Note: In Colab, this may require a runtime restart after installation.\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip -q install kaggle albumentations timm pyyaml umap-learn opencv-python-headless kagglehub scikit-image\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Optional: KaggleHub for dataset download\n",
    "try:\n",
    "    import kagglehub\n",
    "    HAVE_KAGGLEHUB = True\n",
    "except Exception:\n",
    "    HAVE_KAGGLEHUB = False\n",
    "\n",
    "# Optional: skimage for GLCM/LBP\n",
    "try:\n",
    "    from skimage.feature import greycomatrix, greycoprops, local_binary_pattern\n",
    "    HAVE_SKIMAGE = True\n",
    "except Exception:\n",
    "    HAVE_SKIMAGE = False\n",
    "\n",
    "print(\"torch:\", torch.__version__, \"sklearn:\", sklearn.__version__, \"timm:\", timm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e2671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b) Training Configuration (Backbone/Size/Epochs)\n",
    "# Choose your model and training scale here.\n",
    "# Examples: 'efficientnet_b3', 'convnext_tiny', 'resnet50', 'efficientnet_b0'\n",
    "BACKBONE = 'efficientnet_b3'    # or 'convnext_tiny'\n",
    "IMG_SIZE = 352                   # try 320–380 on T4; increase reduces batch size\n",
    "EPOCHS = 50                      # try 40–60 for stronger convergence\n",
    "PATIENCE = 7                     # early stopping patience (epochs without val MAE improvement)\n",
    "\n",
    "# Optional manual batch size override (None -> auto based on IMG_SIZE)\n",
    "BATCH_SIZE_OVERRIDE = None\n",
    "print(f\"Config -> BACKBONE={BACKBONE}, IMG_SIZE={IMG_SIZE}, EPOCHS={EPOCHS}, PATIENCE={PATIENCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Download Dataset from Kaggle\n",
    "# Instructions (Colab):\n",
    "# 1) Create kaggle.json at /content/kaggle.json or upload via Files UI.\n",
    "# 2) Set KAGGLE_DATASET below (e.g., \"owner/dataset-slug\").\n",
    "# 3) Run this cell to download and unzip to /content/data.\n",
    "\n",
    "KAGGLE_DATASET = \"owner/dataset-slug\"  # TODO: replace with your Kaggle dataset slug\n",
    "BASE_DIR = \"/content\" if IN_COLAB else os.getcwd()\n",
    "DATA_ROOT = os.path.join(BASE_DIR, \"data\")\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    # Ensure Kaggle API key\n",
    "    if os.path.exists(\"/content/kaggle.json\"):\n",
    "        os.makedirs(os.path.join(os.path.expanduser(\"~\"), \".kaggle\"), exist_ok=True)\n",
    "        shutil.copy(\"/content/kaggle.json\", os.path.join(os.path.expanduser(\"~\"), \".kaggle\", \"kaggle.json\"))\n",
    "        os.chmod(os.path.join(os.path.expanduser(\"~\"), \".kaggle\", \"kaggle.json\"), 0o600)\n",
    "    else:\n",
    "        print(\"Upload kaggle.json to /content/kaggle.json or set it manually.\")\n",
    "\n",
    "    if KAGGLE_DATASET != \"owner/dataset-slug\":\n",
    "        !kaggle datasets download -d $KAGGLE_DATASET -p $DATA_ROOT -q\n",
    "        !unzip -oq \"$DATA_ROOT/$(basename $KAGGLE_DATASET).zip\" -d $DATA_ROOT\n",
    "        print(\"Downloaded into:\", DATA_ROOT)\n",
    "    else:\n",
    "        print(\"Please set KAGGLE_DATASET to the correct 'owner/dataset' slug.\")\n",
    "else:\n",
    "    print(\"Not in Colab. Ensure your data exists under:\", DATA_ROOT)\n",
    "\n",
    "# Quick check\n",
    "if os.path.exists(DATA_ROOT):\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(DATA_ROOT):\n",
    "        for f in files:\n",
    "            all_files.append(os.path.join(root, f))\n",
    "    img_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "    imgs = [f for f in all_files if os.path.splitext(f)[1].lower() in img_exts]\n",
    "    print(f\"Found {len(all_files)} files; {len(imgs)} images.\")\n",
    "\n",
    "# 3b) Alternative: Download via KaggleHub (FabricNet)\n",
    "# If you prefer not to set kaggle.json, use KaggleHub below.\n",
    "if HAVE_KAGGLEHUB:\n",
    "    try:\n",
    "        hub_path = kagglehub.dataset_download(\"acseckn/fabricnet\")\n",
    "        print(\"KaggleHub path:\", hub_path)\n",
    "        DATA_ROOT = hub_path\n",
    "        # Quick check for images\n",
    "        all_files = []\n",
    "        for root, _, files in os.walk(DATA_ROOT):\n",
    "            for f in files:\n",
    "                all_files.append(os.path.join(root, f))\n",
    "        img_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "        imgs = [f for f in all_files if os.path.splitext(f)[1].lower() in img_exts]\n",
    "        print(f\"(KaggleHub) Found {len(all_files)} files; {len(imgs)} images.\")\n",
    "    except Exception as e:\n",
    "        print(\"KaggleHub download failed:\", e)\n",
    "else:\n",
    "    print(\"KaggleHub not available. Try: pip install kagglehub (on Colab it's auto-installed above)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load CSV and Resolve GSM Target\n",
    "# Configure your CSV and column names here; if previous cell detected them, we reuse.\n",
    "\n",
    "if 'CSV_PATH' not in globals() or CSV_PATH is None:\n",
    "    CSV_PATH = os.path.join(DATA_ROOT, \"metadata.csv\")  # TODO: adjust filename\n",
    "if 'IMAGE_DIR' not in globals() or IMAGE_DIR is None:\n",
    "    IMAGE_DIR = os.path.join(DATA_ROOT, \"images\")       # TODO: adjust folder\n",
    "if 'IMAGE_COL' not in globals() or IMAGE_COL is None:\n",
    "    IMAGE_COL = \"filename\"                               # column in CSV pointing to image file\n",
    "if 'TARGET_COL' not in globals() or TARGET_COL is None:\n",
    "    TARGET_COL = \"gsm\"                                   # preferred target column name after cleaning\n",
    "if 'ALT_MASS_COL' not in globals() or ALT_MASS_COL is None:\n",
    "    ALT_MASS_COL = \"mass\"                                # alternative if dataset uses different name\n",
    "if 'AREA_COL' not in globals() or AREA_COL is None:\n",
    "    AREA_COL = \"area_m2\"                                 # optional area column if available\n",
    "\n",
    "assert os.path.exists(DATA_ROOT), f\"Data root not found: {DATA_ROOT}\"\n",
    "assert os.path.exists(CSV_PATH), f\"CSV not found: {CSV_PATH}\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Normalize image path\n",
    "\n",
    "def resolve_path(row):\n",
    "    p = str(row[IMAGE_COL])\n",
    "    if os.path.isabs(p):\n",
    "        return p\n",
    "    # If IMAGE_DIR is detected, join with it\n",
    "    base_dir = IMAGE_DIR if os.path.exists(IMAGE_DIR) else os.path.dirname(CSV_PATH)\n",
    "    return os.path.join(base_dir, p)\n",
    "\n",
    "# Resolve/derive GSM\n",
    "if TARGET_COL not in df.columns:\n",
    "    # Try to derive GSM from mass/area\n",
    "    if ALT_MASS_COL in df.columns and AREA_COL in df.columns:\n",
    "        # Convert to GSM = mass [g] / area [m^2]\n",
    "        df[TARGET_COL] = df[ALT_MASS_COL] / (df[AREA_COL].replace(0, np.nan))\n",
    "        print(f\"Derived '{TARGET_COL}' from '{ALT_MASS_COL}' and '{AREA_COL}'.\")\n",
    "    elif ALT_MASS_COL in df.columns:\n",
    "        print(f\"Found '{ALT_MASS_COL}'. If units differ, convert to GSM using GSM=mass/area.\")\n",
    "        # Create placeholder; user may annotate missing values\n",
    "        df[TARGET_COL] = df[ALT_MASS_COL]\n",
    "    else:\n",
    "        raise ValueError(\"No GSM or mass column found. Please set TARGET_COL/ALT_MASS_COL correctly.\")\n",
    "\n",
    "# Basic unit hints\n",
    "print(\"Unit hints: 1 mg/cm^2 = 10 g/m^2; 1 g/m^2 = 0.1 mg/cm^2\")\n",
    "\n",
    "# Clean and ensure valid rows\n",
    "\n",
    "df[\"image_path\"] = df.apply(resolve_path, axis=1)\n",
    "df = df[df[\"image_path\"].apply(os.path.exists)].copy()\n",
    "df = df[np.isfinite(df[TARGET_COL])].copy()\n",
    "print(\"Rows after cleanup:\", len(df))\n",
    "\n",
    "# Persist cleaned CSV\n",
    "CLEAN_CSV = os.path.join(DATA_ROOT, \"metadata_clean.csv\")\n",
    "df.to_csv(CLEAN_CSV, index=False)\n",
    "print(\"Saved:\", CLEAN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Train/Val/Test Split with Stratified Target Bins\n",
    "N_BINS = 5\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Create bins for stratification on continuous target\n",
    "bins = pd.qcut(df[TARGET_COL], q=N_BINS, labels=False, duplicates='drop')\n",
    "X_train, X_temp, y_train, y_temp, b_train, b_temp = train_test_split(\n",
    "    df, df[TARGET_COL], bins, test_size=(TEST_SIZE + VAL_SIZE), random_state=RANDOM_STATE, stratify=bins\n",
    ")\n",
    "\n",
    "# Split temp into val/test\n",
    "val_ratio = VAL_SIZE / (TEST_SIZE + VAL_SIZE)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, X_temp[TARGET_COL], test_size=(1 - val_ratio), random_state=RANDOM_STATE,\n",
    "    stratify=pd.qcut(X_temp[TARGET_COL], q=max(2, N_BINS//2), labels=False, duplicates='drop')\n",
    ")\n",
    "\n",
    "SPLIT_DIR = os.path.join(DATA_ROOT, \"splits\")\n",
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "X_train.to_csv(os.path.join(SPLIT_DIR, \"train.csv\"), index=False)\n",
    "X_val.to_csv(os.path.join(SPLIT_DIR, \"val.csv\"), index=False)\n",
    "X_test.to_csv(os.path.join(SPLIT_DIR, \"test.csv\"), index=False)\n",
    "\n",
    "print(len(X_train), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc30b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Preview Images and GSM Distribution\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(df[TARGET_COL], kde=True, ax=axes[0])\n",
    "axes[0].set_title(\"GSM Distribution\")\n",
    "axes[0].set_xlabel(\"GSM (g/m^2)\")\n",
    "\n",
    "sample_paths = X_train[\"image_path\"].sample(min(9, len(X_train)), random_state=42).tolist()\n",
    "cols = 3\n",
    "rows = math.ceil(len(sample_paths)/cols)\n",
    "fig2, axarr = plt.subplots(rows, cols, figsize=(12, 4*rows))\n",
    "axarr = axarr.flatten() if isinstance(axarr, np.ndarray) else [axarr]\n",
    "for i, p in enumerate(sample_paths):\n",
    "    img = cv2.imread(p)\n",
    "    if img is None: continue\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axarr[i].imshow(img)\n",
    "    gsm_val = float(df.loc[df[\"image_path\"]==p, TARGET_COL].values[0])\n",
    "    axarr[i].set_title(f\"{os.path.basename(p)}\\nGSM={gsm_val:.2f}\")\n",
    "    axarr[i].axis('off')\n",
    "for j in range(i+1, len(axarr)):\n",
    "    axarr[j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f7e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Define Augmentation Pipeline (Albumentations) and Visualize\n",
    "IMG_SIZE = globals().get('IMG_SIZE', 256)\n",
    "\n",
    "train_aug = A.Compose([\n",
    "    A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.8, 1.0), ratio=(0.9, 1.1), p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.2, rotate_limit=20, p=0.7),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        A.GaussianBlur(blur_limit=3, p=0.1),\n",
    "    ], p=0.3),\n",
    "    A.OneOf([\n",
    "        A.CLAHE(clip_limit=2.0, p=0.8),\n",
    "        A.RandomBrightnessContrast(p=0.8),\n",
    "        A.HueSaturationValue(p=0.8)\n",
    "    ], p=0.7),\n",
    "    A.Cutout(num_holes=4, max_h_size=IMG_SIZE//10, max_w_size=IMG_SIZE//10, p=0.3),\n",
    "])\n",
    "\n",
    "val_tfms = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE)\n",
    "])\n",
    "\n",
    "\n",
    "def visualize_augments(image_path: str, n: int = 8):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(3*n, 3))\n",
    "    for i in range(n):\n",
    "        aug = train_aug(image=img)\n",
    "        axes[i].imshow(aug[\"image\"]) \n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(\"Augmented variants\")\n",
    "    plt.show()\n",
    "\n",
    "if len(sample_paths) > 0:\n",
    "    visualize_augments(sample_paths[0], n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22451a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Augment to ~1000 Samples with On-Disk Cache\n",
    "TARGET_SIZE = 1000\n",
    "AUG_DIR = os.path.join(DATA_ROOT, \"augmented\")\n",
    "os.makedirs(AUG_DIR, exist_ok=True)\n",
    "\n",
    "train_df = X_train.copy().reset_index(drop=True)\n",
    "cur_n = len(train_df)\n",
    "print(f\"Current train size: {cur_n}\")\n",
    "\n",
    "if cur_n < TARGET_SIZE:\n",
    "    need = TARGET_SIZE - cur_n\n",
    "    # Oversample rows\n",
    "    indices = np.random.choice(train_df.index, size=need, replace=True)\n",
    "    new_rows = []\n",
    "    for idx in tqdm(indices, desc=\"Augmenting\"):\n",
    "        row = train_df.loc[idx]\n",
    "        src = row[\"image_path\"]\n",
    "        img = cv2.imread(src)\n",
    "        if img is None: continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        aug = train_aug(image=img)[\"image\"]\n",
    "        # Save\n",
    "        base = os.path.splitext(os.path.basename(src))[0]\n",
    "        out_name = f\"{base}_aug_{np.random.randint(1e9)}.jpg\"\n",
    "        out_path = os.path.join(AUG_DIR, out_name)\n",
    "        cv2.imwrite(out_path, cv2.cvtColor(aug, cv2.COLOR_RGB2BGR), [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "        r = row.copy()\n",
    "        r[\"image_path\"] = out_path\n",
    "        new_rows.append(r)\n",
    "    if new_rows:\n",
    "        train_df = pd.concat([train_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "print(\"Augmented train size:\", len(train_df))\n",
    "\n",
    "AUG_CSV = os.path.join(DATA_ROOT, \"train_augmented.csv\")\n",
    "train_df.to_csv(AUG_CSV, index=False)\n",
    "print(\"Saved:\", AUG_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cb5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Optional Label Annotation/Correction Widget\n",
    "from ipywidgets import HBox, VBox, Button, FloatText, IntText, Label, Output\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "annot_df = df.copy().reset_index(drop=True)\n",
    "row_idx = IntText(value=0, description='Index:')\n",
    "label_box = FloatText(value=float(annot_df.loc[0, TARGET_COL]), description='GSM:')\n",
    "status = Label(value='')\n",
    "out = Output()\n",
    "\n",
    "btn_prev = Button(description='Prev', button_style='')\n",
    "btn_next = Button(description='Next', button_style='')\n",
    "btn_save = Button(description='Save', button_style='success')\n",
    "\n",
    "\n",
    "def show_row(i):\n",
    "    i = int(np.clip(i, 0, len(annot_df)-1))\n",
    "    row_idx.value = i\n",
    "    label_box.value = float(annot_df.loc[i, TARGET_COL]) if np.isfinite(annot_df.loc[i, TARGET_COL]) else 0.0\n",
    "    img = cv2.imread(annot_df.loc[i, \"image_path\"])\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"{os.path.basename(annot_df.loc[i, 'image_path'])}\\nCurrent GSM={annot_df.loc[i, TARGET_COL]}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def on_prev(_):\n",
    "    show_row(row_idx.value - 1)\n",
    "\n",
    "def on_next(_):\n",
    "    show_row(row_idx.value + 1)\n",
    "\n",
    "def on_save(_):\n",
    "    i = row_idx.value\n",
    "    annot_df.loc[i, TARGET_COL] = float(label_box.value)\n",
    "    status.value = f\"Saved row {i}\"\n",
    "    # persist a versioned CSV\n",
    "    out_csv = os.path.join(DATA_ROOT, \"metadata_annotated.csv\")\n",
    "    annot_df.to_csv(out_csv, index=False)\n",
    "    print(\"Wrote:\", out_csv)\n",
    "    show_row(i)\n",
    "\n",
    "btn_prev.on_click(on_prev)\n",
    "btn_next.on_click(on_next)\n",
    "btn_save.on_click(on_save)\n",
    "\n",
    "ui = VBox([\n",
    "    HBox([row_idx, label_box, btn_prev, btn_next, btn_save]),\n",
    "    status,\n",
    "    out\n",
    "])\n",
    "\n",
    "show_row(0)\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Handcrafted Features and UMAP Visualization\n",
    "import umap\n",
    "\n",
    "# Existing simple features (color moments, histograms, Laplacian/Sobel)\n",
    "def compute_features(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_res = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    feats = []\n",
    "    # Color moments per channel (mean, std, skew)\n",
    "    for c in range(3):\n",
    "        ch = img_res[..., c].astype(np.float32)\n",
    "        mu = ch.mean(); sd = ch.std() + 1e-6\n",
    "        skew = (((ch - mu)/sd)**3).mean()\n",
    "        feats.extend([mu, sd, skew])\n",
    "    # Histogram features\n",
    "    for c in range(3):\n",
    "        hist = cv2.calcHist([img_res],[c],None,[32],[0,256]).flatten()\n",
    "        hist = hist / (hist.sum() + 1e-6)\n",
    "        feats.extend(hist.tolist())\n",
    "    # Texture: Laplacian variance and Sobel energy\n",
    "    gray = cv2.cvtColor(img_res, cv2.COLOR_RGB2GRAY)\n",
    "    lap = cv2.Laplacian(gray, cv2.CV_32F)\n",
    "    feats.append(lap.var())\n",
    "    sobx = cv2.Sobel(gray, cv2.CV_32F, 1, 0)\n",
    "    soby = cv2.Sobel(gray, cv2.CV_32F, 0, 1)\n",
    "    feats.append((np.abs(sobx).mean() + np.abs(soby).mean()))\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "# New: GLCM + LBP features (if skimage is available)\n",
    "def compute_features_glcm_lbp(img_path, distances=(1,2,4), angles=(0, np.pi/4, np.pi/2, 3*np.pi/4)):\n",
    "    if not HAVE_SKIMAGE:\n",
    "        return None\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    gray = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), cv2.COLOR_RGB2GRAY)\n",
    "    gray = cv2.resize(gray, (IMG_SIZE, IMG_SIZE))\n",
    "    # Quantize to 8-bit levels for GLCM\n",
    "    gray_q = (gray / 4).astype(np.uint8)  # reduce levels to 0..63 to limit matrix size\n",
    "    glcm = greycomatrix(gray_q, distances=distances, angles=angles, levels=64, symmetric=True, normed=True)\n",
    "    props = ['contrast','dissimilarity','homogeneity','energy','correlation','ASM']\n",
    "    glcm_feats = []\n",
    "    for p in props:\n",
    "        glcm_feats.extend(greycoprops(glcm, p).ravel().tolist())\n",
    "    # LBP histogram\n",
    "    P, R = 8, 1\n",
    "    lbp = local_binary_pattern(gray, P, R, method='uniform')\n",
    "    n_bins = P + 2\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, n_bins + 1), range=(0, n_bins))\n",
    "    hist = hist.astype('float32'); hist /= (hist.sum() + 1e-6)\n",
    "    return np.concatenate([np.array(glcm_feats, dtype=np.float32), hist], axis=0)\n",
    "\n",
    "# Visualize simple features with UMAP\n",
    "subset = df.sample(min(300, len(df)), random_state=42).reset_index(drop=True)\n",
    "X_feats = []\n",
    "for p in tqdm(subset[\"image_path\"], desc=\"Features-simple\"):\n",
    "    f = compute_features(p)\n",
    "    if f is not None:\n",
    "        X_feats.append(f)\n",
    "X_feats = np.vstack(X_feats)\n",
    "y_feats = subset[TARGET_COL].values[:len(X_feats)]\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "emb = reducer.fit_transform(X_feats)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sc = plt.scatter(emb[:,0], emb[:,1], c=y_feats, cmap='viridis', s=12)\n",
    "plt.title('UMAP of Simple Handcrafted Features (colored by GSM)')\n",
    "plt.colorbar(sc, label='GSM')\n",
    "plt.show()\n",
    "\n",
    "# Visualize GLCM+LBP features (if available)\n",
    "if HAVE_SKIMAGE:\n",
    "    X_feats2 = []\n",
    "    for p in tqdm(subset[\"image_path\"], desc=\"Features-GLCM/LBP\"):\n",
    "        f = compute_features_glcm_lbp(p)\n",
    "        if f is not None:\n",
    "            X_feats2.append(f)\n",
    "    if len(X_feats2) > 10:\n",
    "        X_feats2 = np.vstack(X_feats2)\n",
    "        emb2 = reducer.fit_transform(X_feats2)\n",
    "        plt.figure(figsize=(6,5))\n",
    "        sc2 = plt.scatter(emb2[:,0], emb2[:,1], c=y_feats[:len(X_feats2)], cmap='plasma', s=12)\n",
    "        plt.title('UMAP of GLCM + LBP Features (colored by GSM)')\n",
    "        plt.colorbar(sc2, label='GSM')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"GLCM/LBP features not computed (insufficient images or skimage missing).\")\n",
    "else:\n",
    "    print(\"scikit-image not available: skipping GLCM/LBP features. You can enable by installing scikit-image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf168bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) PyTorch Dataset and DataLoaders\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_aug_torch = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_aug_torch = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "class GSMDataset(Dataset):\n",
    "    def __init__(self, frame: pd.DataFrame, transform=None):\n",
    "        self.frame = frame.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.frame.loc[idx]\n",
    "        p = row[\"image_path\"]\n",
    "        y = float(row[TARGET_COL])\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            img = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "        return img, torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Use augmented train if created\n",
    "if os.path.exists(AUG_CSV):\n",
    "    train_df_for_torch = pd.read_csv(AUG_CSV)\n",
    "else:\n",
    "    train_df_for_torch = X_train\n",
    "\n",
    "val_df_for_torch = X_val\n",
    "test_df_for_torch = X_test\n",
    "\n",
    "train_ds = GSMDataset(train_df_for_torch, transform=train_aug_torch)\n",
    "val_ds   = GSMDataset(val_df_for_torch,   transform=val_aug_torch)\n",
    "test_ds  = GSMDataset(test_df_for_torch,  transform=val_aug_torch)\n",
    "\n",
    "# Auto-tune batch size for larger images unless overridden\n",
    "if 'BATCH_SIZE_OVERRIDE' in globals() and BATCH_SIZE_OVERRIDE is not None:\n",
    "    BATCH_SIZE = int(BATCH_SIZE_OVERRIDE)\n",
    "else:\n",
    "    BATCH_SIZE = 16 if IMG_SIZE >= 320 else 32\n",
    "\n",
    "NUM_WORKERS = 2 if IN_COLAB else 0\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304be07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Define CNN Regressor (Transfer Learning)\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, backbone_name='efficientnet_b0', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
    "        in_feats = self.backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_feats, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        out = self.head(f).squeeze(1)\n",
    "        return out\n",
    "\n",
    "# Use configured backbone\n",
    "BACKBONE = globals().get('BACKBONE', 'efficientnet_b0')\n",
    "model = Regressor(backbone_name=BACKBONE).to(device)\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "\n",
    "# Use configured epochs\n",
    "EPOCHS = globals().get('EPOCHS', 20)\n",
    "warmup_epochs = 2\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, EPOCHS - warmup_epochs))\n",
    "\n",
    "best_val_mae = float('inf')\n",
    "best_ckpt = os.path.join(DATA_ROOT, 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8429306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Training Loop with AMP, Early Stopping, Checkpoints\n",
    "from collections import defaultdict\n",
    "\n",
    "history = defaultdict(list)\n",
    "PATIENCE = globals().get('PATIENCE', 7)\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if torch.cuda.is_available():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            if torch.cuda.is_available():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    preds = model(xb)\n",
    "            else:\n",
    "                preds = model(xb)\n",
    "            val_preds.append(preds.detach().cpu().numpy())\n",
    "            val_trues.append(yb.detach().cpu().numpy())\n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_trues = np.concatenate(val_trues)\n",
    "    val_mae = mean_absolute_error(val_trues, val_preds)\n",
    "\n",
    "    if epoch <= warmup_epochs:\n",
    "        # linear warmup: keep LR small, do not step cosine yet\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 2e-4 * (epoch / max(1, warmup_epochs))\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    history['train_loss'].append(np.mean(train_losses))\n",
    "    history['val_mae'].append(val_mae)\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} - train_loss={np.mean(train_losses):.4f} val_mae={val_mae:.4f} lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    # Early stopping checkpoint\n",
    "    if val_mae < best_val_mae - 1e-4:\n",
    "        best_val_mae = val_mae\n",
    "        no_improve = 0\n",
    "        torch.save({'model': model.state_dict(), 'epoch': epoch}, best_ckpt)\n",
    "        print(\"Saved best checkpoint:\", best_ckpt)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"No improvement: {no_improve}/{PATIENCE}\")\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history['train_loss'], label='train_loss')\n",
    "plt.plot(history['val_mae'], label='val_mae')\n",
    "plt.legend(); plt.title('Training Curves'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Validation Metrics and Diagnostic Plots\n",
    "# Load best checkpoint\n",
    "if os.path.exists(best_ckpt):\n",
    "    ckpt = torch.load(best_ckpt, map_location=device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    print(\"Loaded best model from epoch\", ckpt.get('epoch'))\n",
    "\n",
    "# Evaluate on val set\n",
    "model.eval()\n",
    "val_preds, val_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        if torch.cuda.is_available():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(xb)\n",
    "        else:\n",
    "            preds = model(xb)\n",
    "        val_preds.append(preds.detach().cpu().numpy())\n",
    "        val_trues.append(yb.detach().cpu().numpy())\n",
    "val_preds = np.concatenate(val_preds)\n",
    "val_trues = np.concatenate(val_trues)\n",
    "\n",
    "mae = mean_absolute_error(val_trues, val_preds)\n",
    "rmse = mean_squared_error(val_trues, val_preds, squared=False)\n",
    "r2 = r2_score(val_trues, val_preds)\n",
    "print(f\"Val MAE={mae:.4f} RMSE={rmse:.4f} R2={r2:.4f}\")\n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(val_trues, val_preds, s=10, alpha=0.7)\n",
    "lims = [min(val_trues.min(), val_preds.min()), max(val_trues.max(), val_preds.max())]\n",
    "plt.plot(lims, lims, 'r--')\n",
    "plt.xlabel('True GSM'); plt.ylabel('Pred GSM'); plt.title('Pred vs True (Val)')\n",
    "plt.show()\n",
    "\n",
    "residuals = val_preds - val_trues\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "sns.histplot(residuals, kde=True, ax=axes[0])\n",
    "axes[0].set_title('Residual Histogram')\n",
    "axes[1].scatter(val_trues, residuals, s=10, alpha=0.7)\n",
    "axes[1].axhline(0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('True GSM'); axes[1].set_ylabel('Residual (pred-true)')\n",
    "axes[1].set_title('Residuals vs True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) Grad-CAM and Feature Map Visualizations\n",
    "\n",
    "def grad_cam_on_image(model, image_np_rgb, target_size=IMG_SIZE):\n",
    "    model.eval()\n",
    "    # Preprocess\n",
    "    img = cv2.resize(image_np_rgb, (target_size, target_size))\n",
    "    img_norm = (img/255.0 - np.array(IMAGENET_MEAN)) / np.array(IMAGENET_STD)\n",
    "    tensor = torch.from_numpy(img_norm.transpose(2,0,1)).float().unsqueeze(0).to(device)\n",
    "\n",
    "    feats = None\n",
    "    grads = None\n",
    "\n",
    "    def fw_hook(m, i, o):\n",
    "        nonlocal feats\n",
    "        feats = o\n",
    "    def bw_hook(m, gi, go):\n",
    "        nonlocal grads\n",
    "        grads = go[0]\n",
    "\n",
    "    handle_fw = model.backbone.register_forward_hook(fw_hook)\n",
    "    handle_bw = model.backbone.register_full_backward_hook(bw_hook)\n",
    "\n",
    "    pred = model(tensor)\n",
    "    # For regression, take gradient of output w.r.t. features\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    pred.sum().backward()\n",
    "\n",
    "    handle_fw.remove(); handle_bw.remove()\n",
    "\n",
    "    if feats is None or grads is None:\n",
    "        return None\n",
    "    weights = grads.mean(dim=(2,3), keepdim=True)  # GAP over H,W\n",
    "    cam = (weights * feats).sum(dim=1).squeeze(0)\n",
    "    cam = cam.detach().cpu().numpy()\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / (cam.max() + 1e-6)\n",
    "    cam = cv2.resize(cam, (image_np_rgb.shape[1], image_np_rgb.shape[0]))\n",
    "    heatmap = (plt.cm.jet(cam)[:,:,:3]*255).astype(np.uint8)\n",
    "    overlay = (0.5*image_np_rgb + 0.5*heatmap).astype(np.uint8)\n",
    "    return cam, overlay\n",
    "\n",
    "# Demo Grad-CAM on few validation images\n",
    "samples = val_df_for_torch.sample(min(4, len(val_df_for_torch)), random_state=0)\n",
    "plt.figure(figsize=(10,8))\n",
    "for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "    img = cv2.imread(row['image_path'])\n",
    "    if img is None: continue\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    res = grad_cam_on_image(model, img)\n",
    "    if res is None: continue\n",
    "    cam, overlay = res\n",
    "    plt.subplot(2,2,i)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(f\"Grad-CAM: True GSM={row[TARGET_COL]:.2f}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe35bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) K-Fold Cross-Validation (Optional)\n",
    "RUN_KFOLD = False\n",
    "N_SPLITS = 5\n",
    "\n",
    "# This is a light template; set RUN_KFOLD=True to run (can be time-consuming)\n",
    "if RUN_KFOLD:\n",
    "    folds = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    # Use bins for stratification\n",
    "    bins_all = pd.qcut(df[TARGET_COL], q=5, labels=False, duplicates='drop')\n",
    "    maes = []\n",
    "    for fold, (tr_idx, va_idx) in enumerate(folds.split(df, bins_all)):\n",
    "        tr_df = df.iloc[tr_idx]; va_df = df.iloc[va_idx]\n",
    "        tr_ds = GSMDataset(tr_df, transform=train_aug_torch)\n",
    "        va_ds = GSMDataset(va_df, transform=val_aug_torch)\n",
    "        tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        va_loader = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "        fold_model = Regressor().to(device)\n",
    "        opt = torch.optim.AdamW(fold_model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "        sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "        best = float('inf')\n",
    "        for ep in range(1, 8):\n",
    "            fold_model.train()\n",
    "            for xb, yb in tr_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                if torch.cuda.is_available():\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        pr = fold_model(xb); ls = criterion(pr, yb)\n",
    "                    scaler.scale(ls).backward(); scaler.step(opt); scaler.update()\n",
    "                else:\n",
    "                    pr = fold_model(xb); ls = criterion(pr, yb)\n",
    "                    ls.backward(); opt.step()\n",
    "            # quick val\n",
    "            fold_model.eval()\n",
    "            vp, vt = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    pr = fold_model(xb)\n",
    "                    vp.append(pr.detach().cpu().numpy()); vt.append(yb.detach().cpu().numpy())\n",
    "            vp = np.concatenate(vp); vt = np.concatenate(vt)\n",
    "            mae_fold = mean_absolute_error(vt, vp)\n",
    "            best = min(best, mae_fold)\n",
    "        maes.append(best)\n",
    "        print(f\"Fold {fold}: best MAE {best:.4f}\")\n",
    "    print(\"K-Fold MAE mean +/- std:\", np.mean(maes), np.std(maes))\n",
    "else:\n",
    "    print(\"Set RUN_KFOLD=True to enable cross-validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) Final Test Evaluation and Lightweight Unit Tests\n",
    "# Simple unit checks\n",
    "a_img, a_lbl = train_ds[0]\n",
    "assert a_img.shape[0] == 3 and a_img.ndim == 3, \"Tensor shape mismatch\"\n",
    "assert torch.isfinite(a_img).all(), \"Found non-finite values in image tensor\"\n",
    "assert isinstance(a_lbl.item(), float), \"Label must be float for regression\"\n",
    "\n",
    "# Test evaluation\n",
    "model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pr = model(xb)\n",
    "        test_preds.append(pr.detach().cpu().numpy())\n",
    "        test_trues.append(yb.detach().cpu().numpy())\n",
    "\n",
    "test_preds = np.concatenate(test_preds)\n",
    "test_trues = np.concatenate(test_trues)\n",
    "\n",
    "t_mae = mean_absolute_error(test_trues, test_preds)\n",
    "t_rmse = mean_squared_error(test_trues, test_preds, squared=False)\n",
    "t_r2 = r2_score(test_trues, test_preds)\n",
    "print(f\"Test MAE={t_mae:.4f} RMSE={t_rmse:.4f} R2={t_r2:.4f}\")\n",
    "\n",
    "# Save a CSV of test predictions\n",
    "OUT_DIR = os.path.join(DATA_ROOT, 'outputs')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "TEST_PRED_CSV = os.path.join(OUT_DIR, 'test_predictions.csv')\n",
    "pd.DataFrame({'image_path': test_df_for_torch['image_path'].values, 'true_gsm': test_trues, 'pred_gsm': test_preds}).to_csv(TEST_PRED_CSV, index=False)\n",
    "print('Wrote:', TEST_PRED_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Export Model, Inference Function, and Colab/Drive Integration\n",
    "# Save best model locally\n",
    "FINAL_DIR = os.path.join(DATA_ROOT, 'artifacts')\n",
    "os.makedirs(FINAL_DIR, exist_ok=True)\n",
    "FINAL_MODEL = os.path.join(FINAL_DIR, 'gsm_regressor.pt')\n",
    "\n",
    "if os.path.exists(best_ckpt):\n",
    "    # copy or re-save\n",
    "    torch.save({'model': model.state_dict()}, FINAL_MODEL)\n",
    "    print('Saved:', FINAL_MODEL)\n",
    "\n",
    "# Optionally save to Drive when in Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_OUT = \"/content/drive/MyDrive/gsm_artifacts\"\n",
    "    os.makedirs(DRIVE_OUT, exist_ok=True)\n",
    "    !cp -v \"$FINAL_MODEL\" \"$DRIVE_OUT/\"\n",
    "\n",
    "# Inference utilities\n",
    "@torch.no_grad()\n",
    "def predict_image(path: str) -> float:\n",
    "    model.eval()\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = (img/255.0 - np.array(IMAGENET_MEAN)) / np.array(IMAGENET_STD)\n",
    "    ten = torch.from_numpy(img.transpose(2,0,1)).float().unsqueeze(0).to(device)\n",
    "    if torch.cuda.is_available():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pr = model(ten)\n",
    "    else:\n",
    "        pr = model(ten)\n",
    "    return float(pr.squeeze().detach().cpu().item())\n",
    "\n",
    "# Demo inference on a few test images\n",
    "for p in test_df_for_torch['image_path'].sample(min(3, len(test_df_for_torch)), random_state=7):\n",
    "    pred = predict_image(p)\n",
    "    print(os.path.basename(p), '-> Pred GSM =', round(pred, 3))\n",
    "\n",
    "# Save minimal requirements for reproducibility\n",
    "REQ_TXT = os.path.join(FINAL_DIR, 'requirements.txt')\n",
    "with open(REQ_TXT, 'w') as f:\n",
    "    f.write('\\n'.join([\n",
    "        'torch', 'torchvision', 'timm', 'albumentations', 'opencv-python-headless',\n",
    "        'pandas', 'numpy', 'scikit-learn', 'umap-learn', 'matplotlib', 'seaborn', 'tqdm'\n",
    "    ]))\n",
    "print('Saved requirements:', REQ_TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f20a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Scikit-learn on Handcrafted Features\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Build features for train/val jointly to train baseline\n",
    "base_df = pd.concat([X_train, X_val], ignore_index=True)\n",
    "Xf, yf = [], []\n",
    "for _, r in tqdm(base_df.iterrows(), total=len(base_df), desc='Baseline features'):\n",
    "    f = compute_features(r['image_path'])\n",
    "    if f is not None:\n",
    "        Xf.append(f); yf.append(float(r[TARGET_COL]))\n",
    "Xf = np.vstack(Xf); yf = np.array(yf)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "cv = 5\n",
    "scores = cross_val_score(rf, Xf, yf, cv=cv, scoring='neg_mean_absolute_error')\n",
    "print(f\"Baseline RF {cv}-fold MAE: {(-scores).mean():.4f} +/- {(-scores).std():.4f}\")\n",
    "rf.fit(Xf, yf)\n",
    "\n",
    "# Eval on test\n",
    "Xt, yt = [], []\n",
    "for _, r in tqdm(X_test.iterrows(), total=len(X_test), desc='Baseline test features'):\n",
    "    f = compute_features(r['image_path'])\n",
    "    if f is not None:\n",
    "        Xt.append(f); yt.append(float(r[TARGET_COL]))\n",
    "Xt = np.vstack(Xt); yt = np.array(yt)\n",
    "yp = rf.predict(Xt)\n",
    "print(\"Baseline Test MAE=%.4f RMSE=%.4f R2=%.4f\" % (\n",
    "    mean_absolute_error(yt, yp), mean_squared_error(yt, yp, squared=False), r2_score(yt, yp)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa1c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c) Autodetect CSV, image directory, and key columns (after download)\n",
    "import glob\n",
    "\n",
    "# Try to find a CSV under DATA_ROOT\n",
    "csv_candidates = glob.glob(os.path.join(DATA_ROOT, \"**\", \"*.csv\"), recursive=True)\n",
    "CSV_PATH = csv_candidates[0] if csv_candidates else None\n",
    "print(\"Detected CSV:\", CSV_PATH)\n",
    "\n",
    "# Heuristic: find an image directory by scanning for many images\n",
    "img_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "image_files = [p for p in glob.glob(os.path.join(DATA_ROOT, \"**\", \"*\"), recursive=True)\n",
    "               if os.path.splitext(p)[1].lower() in img_exts]\n",
    "\n",
    "from collections import Counter\n",
    "parent_counts = Counter([os.path.dirname(p) for p in image_files])\n",
    "IMAGE_DIR = None\n",
    "if parent_counts:\n",
    "    IMAGE_DIR = max(parent_counts.items(), key=lambda kv: kv[1])[0]\n",
    "print(\"Detected IMAGE_DIR:\", IMAGE_DIR)\n",
    "\n",
    "# Detect likely image and target columns\n",
    "IMAGE_COL = None\n",
    "TARGET_COL = 'gsm'\n",
    "ALT_MASS_COL = 'mass'\n",
    "AREA_COL = 'area_m2'\n",
    "\n",
    "if CSV_PATH and os.path.exists(CSV_PATH):\n",
    "    tmp = pd.read_csv(CSV_PATH)\n",
    "    cols = [c.lower() for c in tmp.columns]\n",
    "    # Image column heuristic\n",
    "    for key in [\"image\", \"file\", \"filename\", \"path\"]:\n",
    "        matches = [c for c in tmp.columns if key in c.lower()]\n",
    "        if matches:\n",
    "            IMAGE_COL = matches[0]\n",
    "            break\n",
    "    # Target column heuristic\n",
    "    gsm_like = [c for c in tmp.columns if \"gsm\" in c.lower() or \"grammage\" in c.lower() or \"basis\" in c.lower()]\n",
    "    if gsm_like:\n",
    "        TARGET_COL = gsm_like[0]\n",
    "    else:\n",
    "        # mass-like fallback\n",
    "        mass_like = [c for c in tmp.columns if any(k in c.lower() for k in [\"mass\",\"weight\",\"wt\"]) ]\n",
    "        if mass_like:\n",
    "            ALT_MASS_COL = mass_like[0]\n",
    "        # area-like fallback\n",
    "        area_like = [c for c in tmp.columns if \"area\" in c.lower()]\n",
    "        if area_like:\n",
    "            AREA_COL = area_like[0]\n",
    "\n",
    "print(\"IMAGE_COL:\", IMAGE_COL, \"TARGET_COL:\", TARGET_COL, \"ALT_MASS_COL:\", ALT_MASS_COL, \"AREA_COL:\", AREA_COL)\n",
    "\n",
    "# Persist detected settings for downstream cells\n",
    "DETECTED_SETTINGS = {\n",
    "    'CSV_PATH': CSV_PATH,\n",
    "    'IMAGE_DIR': IMAGE_DIR,\n",
    "    'IMAGE_COL': IMAGE_COL,\n",
    "    'TARGET_COL': TARGET_COL,\n",
    "    'ALT_MASS_COL': ALT_MASS_COL,\n",
    "    'AREA_COL': AREA_COL,\n",
    "}\n",
    "print(\"Settings:\", DETECTED_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a447bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10d) Deep Embeddings (timm backbone) + UMAP\n",
    "# Extract penultimate-layer embeddings from a pretrained backbone and visualize separability\n",
    "import umap\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_backbone_embeddings(paths, backbone_name='efficientnet_b0'):\n",
    "    model_fe = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool='avg').to(device).eval()\n",
    "    embs = []\n",
    "    for p in tqdm(paths, desc='Embeddings'):\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = (img/255.0 - np.array(IMAGENET_MEAN)) / np.array(IMAGENET_STD)\n",
    "        ten = torch.from_numpy(img.transpose(2,0,1)).float().unsqueeze(0).to(device)\n",
    "        if torch.cuda.is_available():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                f = model_fe(ten)\n",
    "        else:\n",
    "            f = model_fe(ten)\n",
    "        embs.append(f.detach().cpu().numpy())\n",
    "    if len(embs) == 0:\n",
    "        return None\n",
    "    return np.vstack(embs)\n",
    "\n",
    "subset_deep = df.sample(min(400, len(df)), random_state=7).reset_index(drop=True)\n",
    "X_deep = extract_backbone_embeddings(subset_deep['image_path'].tolist())\n",
    "if X_deep is not None:\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "    embd = reducer.fit_transform(X_deep)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sc = plt.scatter(embd[:,0], embd[:,1], c=subset_deep[TARGET_COL].values[:len(embd)], cmap='viridis', s=10)\n",
    "    plt.title('UMAP of Deep Embeddings (pretrained backbone)')\n",
    "    plt.colorbar(sc, label='GSM')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No embeddings extracted (no readable images).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00612fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10e) Inspect Feature Types, Names, and Example Values\n",
    "# This cell prints the names for each feature we extract and shows sample values.\n",
    "\n",
    "def get_simple_feature_names(img_size=IMG_SIZE, hist_bins=32):\n",
    "    names = []\n",
    "    for c in range(3):\n",
    "        names += [f\"c{c}_mean\", f\"c{c}_std\", f\"c{c}_skew\"]\n",
    "    for c in range(3):\n",
    "        for b in range(hist_bins):\n",
    "            names.append(f\"c{c}_hist_bin{b}\")\n",
    "    names += [\"laplacian_var\", \"sobel_energy\"]\n",
    "    return names\n",
    "\n",
    "def get_glcm_lbp_feature_names(distances=(1,2,4), angles=(0, np.pi/4, np.pi/2, 3*np.pi/4)):\n",
    "    props = ['contrast','dissimilarity','homogeneity','energy','correlation','ASM']\n",
    "    names = []\n",
    "    for p in props:\n",
    "        for d in distances:\n",
    "            for a in angles:\n",
    "                adeg = int(np.rad2deg(a))\n",
    "                names.append(f\"glcm_{p}_d{d}_a{adeg}\")\n",
    "    # LBP histogram bins: P+2 (with P=8 in compute_features_glcm_lbp)\n",
    "    P = 8\n",
    "    n_bins = P + 2\n",
    "    for b in range(n_bins):\n",
    "        names.append(f\"lbp_bin{b}\")\n",
    "    return names\n",
    "\n",
    "# Print simple feature info\n",
    "simple_names = get_simple_feature_names()\n",
    "print(f\"Simple features count: {len(simple_names)}\")\n",
    "print(\"Simple features (first 20):\", simple_names[:20], \"...\")\n",
    "\n",
    "# Show example values for one image\n",
    "if len(df) > 0:\n",
    "    ex_path = df.iloc[0]['image_path']\n",
    "    fv = compute_features(ex_path)\n",
    "    if fv is not None:\n",
    "        print(f\"Example simple feature vector length: {len(fv)}\")\n",
    "        for n, v in list(zip(simple_names, fv))[:20]:\n",
    "            print(f\"  {n}: {float(v):.4f}\")\n",
    "\n",
    "# Print texture feature info if available\n",
    "if 'compute_features_glcm_lbp' in globals() and HAVE_SKIMAGE:\n",
    "    glcm_lbp_names = get_glcm_lbp_feature_names()\n",
    "    print(f\"GLCM+LBP features count: {len(glcm_lbp_names)}\")\n",
    "    print(\"GLCM+LBP features (first 20):\", glcm_lbp_names[:20], \"...\")\n",
    "    if len(df) > 0:\n",
    "        ex_path = df.iloc[0]['image_path']\n",
    "        fv2 = compute_features_glcm_lbp(ex_path)\n",
    "        if fv2 is not None:\n",
    "            print(f\"Example GLCM+LBP feature vector length: {len(fv2)}\")\n",
    "            for n, v in list(zip(glcm_lbp_names, fv2))[:20]:\n",
    "                print(f\"  {n}: {float(v):.4f}\")\n",
    "else:\n",
    "    print(\"GLCM/LBP not available; install scikit-image to enable.\")\n",
    "\n",
    "# Deep embedding dimension (from pretrained backbone)\n",
    "try:\n",
    "    tmp_backbone = timm.create_model('efficientnet_b0', pretrained=True, num_classes=0, global_pool='avg')\n",
    "    print(\"Deep embedding (efficientnet_b0) dim:\", tmp_backbone.num_features)\n",
    "except Exception as e:\n",
    "    print(\"Could not inspect deep embedding dim:\", e)\n",
    "\n",
    "# Optional: quick feature importance on simple features\n",
    "RUN_FEATURE_IMPORTANCE = False\n",
    "if RUN_FEATURE_IMPORTANCE:\n",
    "    Xf, yf = [], []\n",
    "    for _, r in tqdm(df.sample(min(300, len(df)), random_state=0).iterrows(), total=min(300, len(df)), desc='Imp feats'):\n",
    "        f = compute_features(r['image_path'])\n",
    "        if f is not None:\n",
    "            Xf.append(f); yf.append(float(r[TARGET_COL]))\n",
    "    if Xf:\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        Xf = np.vstack(Xf); yf = np.array(yf)\n",
    "        rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "        rf.fit(Xf, yf)\n",
    "        importances = rf.feature_importances_\n",
    "        idx = np.argsort(importances)[::-1][:20]\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.barh([simple_names[i] for i in idx][::-1], importances[idx][::-1])\n",
    "        plt.title('Top-20 Feature Importances (Simple Features)')\n",
    "        plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
